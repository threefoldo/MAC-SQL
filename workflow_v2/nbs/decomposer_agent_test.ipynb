{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Decomposer Agent Tool Test\n",
    "\n",
    "This notebook demonstrates how to use the `MemoryAgentTool` with our KeyValueMemory implementation to create a decomposer agent that can read from and write to memory.\n",
    "\n",
    "The decomposer agent breaks down complex queries into simpler sub-queries that can be more easily processed by downstream agents.\n",
    "\n",
    "The `MemoryAgentTool` class provides:\n",
    "1. **Memory integration** for agents by extending BaseTool\n",
    "2. **Pre-processing** via reader callbacks that fetch relevant context from memory before agent execution\n",
    "3. **Post-processing** via parser callbacks that extract and store information from agent outputs after execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Reduce noise from autogen\n",
    "logging.getLogger('autogen_core').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our KeyValueMemory and MemoryAgentTool\n",
    "from memory import KeyValueMemory\n",
    "from memory_agent_tool import MemoryAgentTool, MemoryAgentToolArgs\n",
    "\n",
    "# Import necessary AutoGen components\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.base import TaskResult\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up our memory store and model client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the shared memory store\n",
    "memory = KeyValueMemory(name=\"text_to_sql_memory\")\n",
    "\n",
    "# Set up the model client - replace with your specific model\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",  # or other appropriate model\n",
    "    temperature=0.1,\n",
    "    timeout=120\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define memory callback functions for our decomposer agent\n",
    "\n",
    "The agent will have custom reader and parser functions that define how it interacts with memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposer Agent memory callbacks\n",
    "async def decomposer_reader(memory, task, cancellation_token):\n",
    "    \"\"\"Read relevant information for the decomposer agent.\"\"\"\n",
    "    print(\"üîç READER: Starting decomposer_reader function\")\n",
    "    context = {}\n",
    "    \n",
    "    # Get database schema if available\n",
    "    schema = await memory.get(\"current_schema\") or await memory.get(\"full_database_schema\")\n",
    "    if schema:\n",
    "        print(f\"üîç READER: Found schema (length: {len(schema)})\")\n",
    "        context[\"schema\"] = schema\n",
    "    else:\n",
    "        print(\"üîç READER: No schema found in memory\")\n",
    "    \n",
    "    # Get query history for context\n",
    "    query_history_json = await memory.get(\"query_history\")\n",
    "    if query_history_json:\n",
    "        try:\n",
    "            query_history = json.loads(query_history_json)\n",
    "            if query_history:\n",
    "                print(f\"üîç READER: Found query history ({len(query_history)} entries)\")\n",
    "                context[\"query_history\"] = query_history\n",
    "            else:\n",
    "                print(\"üîç READER: Query history is empty\")\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(\"Failed to parse query history JSON\")\n",
    "            print(\"üîç READER: Error parsing query history JSON\")\n",
    "    else:\n",
    "        print(\"üîç READER: No query history found in memory\")\n",
    "    \n",
    "    # Get previous decompositions if available\n",
    "    decompositions_json = await memory.get(\"previous_decompositions\")\n",
    "    if decompositions_json:\n",
    "        try:\n",
    "            decompositions = json.loads(decompositions_json)\n",
    "            if decompositions:\n",
    "                print(f\"üîç READER: Found previous decompositions ({len(decompositions)} entries)\")\n",
    "                context[\"previous_decompositions\"] = decompositions\n",
    "            else:\n",
    "                print(\"üîç READER: Previous decompositions list is empty\")\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(\"Failed to parse previous decompositions JSON\")\n",
    "            print(\"üîç READER: Error parsing previous decompositions JSON\")\n",
    "    else:\n",
    "        print(\"üîç READER: No previous decompositions found in memory\")\n",
    "    \n",
    "    print(f\"üîç READER: Returning context with keys: {list(context.keys())}\")\n",
    "    return context\n",
    "\n",
    "async def decomposer_parser(memory, task, result, cancellation_token):\n",
    "    \"\"\"Parse and store decomposition results.\"\"\"\n",
    "    print(\"üîç PARSER: Starting decomposer_parser function\")\n",
    "    \n",
    "    if not result.messages:\n",
    "        print(\"üîç PARSER: No messages in result\")\n",
    "        return\n",
    "        \n",
    "    last_message = result.messages[-1].content\n",
    "    print(f\"üîç PARSER: Processing message of length: {len(last_message)}\")\n",
    "    print(f\"üîç PARSER: Message preview: {last_message[:100]}...\")\n",
    "    \n",
    "    # Look for decomposition in XML format - handle both plain XML and XML inside code blocks\n",
    "    # First try finding XML directly\n",
    "    decomposition_match = re.search(r'<decomposition>.*?</decomposition>', last_message, re.DOTALL)\n",
    "    if decomposition_match:\n",
    "        print(\"üîç PARSER: Found direct XML match\")\n",
    "    else:\n",
    "        print(\"üîç PARSER: No direct XML match found, checking for code blocks\")\n",
    "        # If not found, try finding it inside markdown code blocks\n",
    "        xml_in_code = re.search(r'```(?:xml)?\\s*(<decomposition>.*?</decomposition>)\\s*```', last_message, re.DOTALL)\n",
    "        if xml_in_code:\n",
    "            print(\"üîç PARSER: Found XML in code block\")\n",
    "            decomposition_match = re.search(r'<decomposition>.*?</decomposition>', xml_in_code.group(1), re.DOTALL)\n",
    "            if decomposition_match:\n",
    "                print(\"üîç PARSER: Successfully extracted XML from code block\")\n",
    "            else:\n",
    "                print(\"üîç PARSER: Failed to extract XML from code block\")\n",
    "        else:\n",
    "            print(\"üîç PARSER: No XML in code block found\")\n",
    "    \n",
    "    if decomposition_match:\n",
    "        decomposition_str = decomposition_match.group()\n",
    "        print(f\"üîç PARSER: Extracted decomposition of length: {len(decomposition_str)}\")\n",
    "        await memory.set(\"current_decomposition\", decomposition_str)\n",
    "        print(\"üîç PARSER: Stored decomposition in memory\")\n",
    "        \n",
    "        # Extract sub-queries for easier access\n",
    "        subqueries = []\n",
    "        subquery_matches = re.findall(r'<subquery.*?>(.+?)</subquery>', decomposition_str, re.DOTALL)\n",
    "        if subquery_matches:\n",
    "            print(f\"üîç PARSER: Found {len(subquery_matches)} subqueries\")\n",
    "            for i, subquery in enumerate(subquery_matches):\n",
    "                subqueries.append({\n",
    "                    \"id\": f\"subquery_{i+1}\",\n",
    "                    \"text\": subquery.strip()\n",
    "                })\n",
    "            await memory.set(\"subqueries\", json.dumps(subqueries))\n",
    "            print(f\"üîç PARSER: Stored {len(subqueries)} subqueries in memory\")\n",
    "        else:\n",
    "            print(\"üîç PARSER: No subqueries found in decomposition\")\n",
    "    else:\n",
    "        print(\"üîç PARSER: No decomposition found in result\")\n",
    "        \n",
    "    # Parse the original query\n",
    "    query = \"\"\n",
    "    try:\n",
    "        task_obj = json.loads(task)\n",
    "        query = task_obj.get(\"query\", task)\n",
    "        print(f\"üîç PARSER: Extracted query from JSON task: {query[:50]}...\")\n",
    "    except json.JSONDecodeError:\n",
    "        query = task\n",
    "        print(f\"üîç PARSER: Using raw task as query: {query[:50]}...\")\n",
    "        \n",
    "    # Store decomposition in history\n",
    "    decompositions = []\n",
    "    decompositions_json = await memory.get(\"previous_decompositions\")\n",
    "    if decompositions_json:\n",
    "        try:\n",
    "            decompositions = json.loads(decompositions_json)\n",
    "            print(f\"üîç PARSER: Loaded existing decomposition history with {len(decompositions)} entries\")\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(\"Failed to parse previous decompositions, creating new list\")\n",
    "            print(\"üîç PARSER: Error parsing previous decompositions, creating new list\")\n",
    "    else:\n",
    "        print(\"üîç PARSER: No existing decomposition history, creating new list\")\n",
    "    \n",
    "    # Add current decomposition to history\n",
    "    decompositions.append({\n",
    "        \"query\": query,\n",
    "        \"decomposition\": last_message,\n",
    "        \"timestamp\": str(datetime.datetime.now())\n",
    "    })\n",
    "    print(f\"üîç PARSER: Added current decomposition to history (now {len(decompositions)} entries)\")\n",
    "    \n",
    "    # Keep only the last 5 decompositions\n",
    "    if len(decompositions) > 5:\n",
    "        decompositions = decompositions[-5:]\n",
    "        print(f\"üîç PARSER: Trimmed history to last 5 entries\")\n",
    "        \n",
    "    await memory.set(\"previous_decompositions\", json.dumps(decompositions))\n",
    "    print(\"üîç PARSER: Updated decomposition history in memory\")\n",
    "    \n",
    "    # Verify memory was updated\n",
    "    current_decomp = await memory.get(\"current_decomposition\")\n",
    "    subqueries_json = await memory.get(\"subqueries\")\n",
    "    decomp_history = await memory.get(\"previous_decompositions\")\n",
    "    \n",
    "    if current_decomp:\n",
    "        print(\"üîç PARSER: Verified current_decomposition was saved\")\n",
    "    else:\n",
    "        print(\"üîç PARSER: WARNING: current_decomposition not found after saving!\")\n",
    "        \n",
    "    if subqueries_json:\n",
    "        print(\"üîç PARSER: Verified subqueries were saved\")\n",
    "    else:\n",
    "        print(\"üîç PARSER: WARNING: subqueries not found after saving!\")\n",
    "        \n",
    "    if decomp_history:\n",
    "        print(\"üîç PARSER: Verified decomposition history was saved\")\n",
    "    else:\n",
    "        print(\"üîç PARSER: WARNING: decomposition history not found after saving!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create our Decomposer Agent with System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define system message for the decomposer agent\n",
    "DECOMPOSER_SYSTEM_MESSAGE = \"\"\"\n",
    "You are a query decomposition expert. Your role is to:\n",
    "1. Analyze complex natural language queries\n",
    "2. Break them down into simpler sub-queries\n",
    "3. Return the decomposition in XML format\n",
    "\n",
    "For each input query, return a decomposition with one or more subqueries. Each subquery should:\n",
    "- Be simpler than the original query\n",
    "- Focus on a single aspect of the original query\n",
    "- Be answerable using the database schema provided\n",
    "- Include a description of how results will be combined\n",
    "\n",
    "ALWAYS return your decomposition in this XML format:\n",
    "<decomposition>\n",
    "  <original_query>The original query text</original_query>\n",
    "  <subquery id=\"1\">First simpler subquery</subquery>\n",
    "  <subquery id=\"2\">Second simpler subquery</subquery>\n",
    "  <combination_logic>\n",
    "    Explanation of how to combine the subquery results to answer the original query\n",
    "  </combination_logic>\n",
    "</decomposition>\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "decomposer_agent = AssistantAgent(\n",
    "    name=\"decomposer\",\n",
    "    system_message=DECOMPOSER_SYSTEM_MESSAGE,\n",
    "    model_client=model_client,\n",
    "    description=\"Breaks down complex queries into simpler sub-queries\"\n",
    ")\n",
    "\n",
    "# Wrap the agent with memory capabilities\n",
    "decomposer_tool = MemoryAgentTool(\n",
    "    agent=decomposer_agent,\n",
    "    memory=memory,\n",
    "    reader_callback=decomposer_reader,\n",
    "    parser_callback=decomposer_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up Sample Database Schema\n",
    "\n",
    "We'll use the same sample database schema as in the schema selector test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:20:53,046 - root - INFO - [KeyValueMemory] Memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full database schema stored in memory\n",
      "Decomposition history initialized\n"
     ]
    }
   ],
   "source": [
    "# Reset memory for a fresh run\n",
    "await memory.clear()\n",
    "\n",
    "# Set up a sample database schema\n",
    "full_schema = \"\"\"\n",
    "<database_schema>\n",
    "  <table name=\"customers\">\n",
    "    <column name=\"customer_id\" type=\"INTEGER\" primary_key=\"true\" />\n",
    "    <column name=\"name\" type=\"TEXT\" />\n",
    "    <column name=\"email\" type=\"TEXT\" />\n",
    "    <column name=\"join_date\" type=\"DATE\" />\n",
    "  </table>\n",
    "  <table name=\"orders\">\n",
    "    <column name=\"order_id\" type=\"INTEGER\" primary_key=\"true\" />\n",
    "    <column name=\"customer_id\" type=\"INTEGER\" foreign_key=\"customers.customer_id\" />\n",
    "    <column name=\"order_date\" type=\"DATE\" />\n",
    "    <column name=\"total_amount\" type=\"DECIMAL\" />\n",
    "  </table>\n",
    "  <table name=\"products\">\n",
    "    <column name=\"product_id\" type=\"INTEGER\" primary_key=\"true\" />\n",
    "    <column name=\"name\" type=\"TEXT\" />\n",
    "    <column name=\"price\" type=\"DECIMAL\" />\n",
    "    <column name=\"category\" type=\"TEXT\" />\n",
    "  </table>\n",
    "  <table name=\"order_items\">\n",
    "    <column name=\"item_id\" type=\"INTEGER\" primary_key=\"true\" />\n",
    "    <column name=\"order_id\" type=\"INTEGER\" foreign_key=\"orders.order_id\" />\n",
    "    <column name=\"product_id\" type=\"INTEGER\" foreign_key=\"products.product_id\" />\n",
    "    <column name=\"quantity\" type=\"INTEGER\" />\n",
    "    <column name=\"price\" type=\"DECIMAL\" />\n",
    "  </table>\n",
    "  <table name=\"inventory\">\n",
    "    <column name=\"inventory_id\" type=\"INTEGER\" primary_key=\"true\" />\n",
    "    <column name=\"product_id\" type=\"INTEGER\" foreign_key=\"products.product_id\" />\n",
    "    <column name=\"quantity\" type=\"INTEGER\" />\n",
    "    <column name=\"warehouse\" type=\"TEXT\" />\n",
    "  </table>\n",
    "</database_schema>\n",
    "\"\"\"\n",
    "\n",
    "# Store the schema\n",
    "await memory.set(\"full_database_schema\", full_schema)\n",
    "print(\"Full database schema stored in memory\")\n",
    "\n",
    "# Initialize empty decomposition history\n",
    "await memory.set(\"previous_decompositions\", json.dumps([]))\n",
    "print(\"Decomposition history initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Decomposer Agent for a Complex Query\n",
    "\n",
    "Let's test our decomposer with a complex query that should be broken down into simpler parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç READER: Starting decomposer_reader function\n",
      "üîç READER: Found schema (length: 1406)\n",
      "üîç READER: No query history found in memory\n",
      "üîç READER: Previous decompositions list is empty\n",
      "üîç READER: Returning context with keys: ['schema']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:20:58,100 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PARSER: Starting decomposer_parser function\n",
      "üîç PARSER: Processing message of length: 969\n",
      "üîç PARSER: Message preview: ```xml\n",
      "<decomposition>\n",
      "  <original_query>Find the top 3 product categories by revenue and for each c...\n",
      "üîç PARSER: Found direct XML match\n",
      "üîç PARSER: Extracted decomposition of length: 958\n",
      "üîç PARSER: Stored decomposition in memory\n",
      "üîç PARSER: Found 2 subqueries\n",
      "üîç PARSER: Stored 2 subqueries in memory\n",
      "üîç PARSER: Extracted query from JSON task: Find the top 3 product categories by revenue and f...\n",
      "üîç PARSER: Loaded existing decomposition history with 0 entries\n",
      "üîç PARSER: Added current decomposition to history (now 1 entries)\n",
      "üîç PARSER: Updated decomposition history in memory\n",
      "üîç PARSER: Verified current_decomposition was saved\n",
      "üîç PARSER: Verified subqueries were saved\n",
      "üîç PARSER: Verified decomposition history was saved\n",
      "\n",
      "Agent Response:\n",
      "```xml\n",
      "<decomposition>\n",
      "  <original_query>Find the top 3 product categories by revenue and for each category, show the customer who spent the most on that category</original_query>\n",
      "  <subquery id=\"1\">Calculate the total revenue for each product category by summing the price times quantity for all order items, and then find the top 3 categories with the highest revenue.</subquery>\n",
      "  <subquery id=\"2\">For each of the top 3 categories identified in subquery 1, determine the customer who spent the most by summing the total spending per customer on products within that category.</subquery>\n",
      "  <combination_logic>\n",
      "    First, execute subquery 1 to identify the top 3 product categories by revenue. Then, for each of these categories, execute subquery 2 to find the customer who spent the most on that category. Combine the results to provide a list of the top 3 categories along with the top-spending customer for each category.\n",
      "  </combination_logic>\n",
      "</decomposition>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Define a complex query that needs decomposition\n",
    "complex_query = \"Find the top 3 product categories by revenue and for each category, show the customer who spent the most on that category\"\n",
    "\n",
    "# Create a task with the schema and query\n",
    "task1 = json.dumps({\n",
    "    \"query\": complex_query,\n",
    "    \"schema\": await memory.get(\"full_database_schema\")\n",
    "})\n",
    "\n",
    "# Create a cancellation token\n",
    "cancellation_token = CancellationToken()\n",
    "\n",
    "# Create the proper arguments object\n",
    "args = MemoryAgentToolArgs(task=task1)\n",
    "\n",
    "# Run the agent\n",
    "result1 = await decomposer_tool.run(\n",
    "    args=args,\n",
    "    cancellation_token=cancellation_token\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(f\"\\nAgent Response:\\n{result1.messages[-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Decomposer Agent for Another Complex Query\n",
    "\n",
    "Let's run another query to demonstrate memory continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç READER: Starting decomposer_reader function\n",
      "üîç READER: Found schema (length: 1406)\n",
      "üîç READER: No query history found in memory\n",
      "üîç READER: Found previous decompositions (1 entries)\n",
      "üîç READER: Returning context with keys: ['schema', 'previous_decompositions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:21:01,540 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PARSER: Starting decomposer_parser function\n",
      "üîç PARSER: Processing message of length: 922\n",
      "üîç PARSER: Message preview: ```xml\n",
      "<decomposition>\n",
      "  <original_query>Compare the average order amount between customers who join...\n",
      "üîç PARSER: Found direct XML match\n",
      "üîç PARSER: Extracted decomposition of length: 911\n",
      "üîç PARSER: Stored decomposition in memory\n",
      "üîç PARSER: Found 2 subqueries\n",
      "üîç PARSER: Stored 2 subqueries in memory\n",
      "üîç PARSER: Extracted query from JSON task: Compare the average order amount between customers...\n",
      "üîç PARSER: Loaded existing decomposition history with 1 entries\n",
      "üîç PARSER: Added current decomposition to history (now 2 entries)\n",
      "üîç PARSER: Updated decomposition history in memory\n",
      "üîç PARSER: Verified current_decomposition was saved\n",
      "üîç PARSER: Verified subqueries were saved\n",
      "üîç PARSER: Verified decomposition history was saved\n",
      "\n",
      "Agent Response:\n",
      "```xml\n",
      "<decomposition>\n",
      "  <original_query>Compare the average order amount between customers who joined before 2020 and those who joined after, broken down by product category</original_query>\n",
      "  <subquery id=\"1\">Identify customers who joined before 2020 and calculate the average order amount for each product category for these customers.</subquery>\n",
      "  <subquery id=\"2\">Identify customers who joined after 2020 and calculate the average order amount for each product category for these customers.</subquery>\n",
      "  <combination_logic>\n",
      "    Execute subquery 1 to get the average order amounts for customers who joined before 2020, broken down by product category. Execute subquery 2 to get the same information for customers who joined after 2020. Compare the results from both subqueries to analyze differences in average order amounts between the two groups for each product category.\n",
      "  </combination_logic>\n",
      "</decomposition>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Define another complex query\n",
    "complex_query2 = \"Compare the average order amount between customers who joined before 2020 and those who joined after, broken down by product category\"\n",
    "\n",
    "# Create a task with the schema and query\n",
    "task2 = json.dumps({\n",
    "    \"query\": complex_query2,\n",
    "    \"schema\": await memory.get(\"full_database_schema\")\n",
    "})\n",
    "\n",
    "# Create the proper arguments object\n",
    "args = MemoryAgentToolArgs(task=task2)\n",
    "\n",
    "# Run the agent again\n",
    "result2 = await decomposer_tool.run(\n",
    "    args=args,\n",
    "    cancellation_token=cancellation_token\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(f\"\\nAgent Response:\\n{result2.messages[-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Memory Contents\n",
    "\n",
    "Let's examine what's stored in memory after our agent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current decomposition:\n",
      "<decomposition>\n",
      "  <original_query>Compare the average order amount between customers who joined before 2020 and those who joined after, broken down by product category</original_query>\n",
      "  <subquery id=\"1\">Identify customers who joined before 2020 and calculate the average order amount for each product category for these customers.</subquery>\n",
      "  <subquery id=\"2\">Identify customers who joined after 2020 and calculate the average order amount for each product category for these customers.</subquery>\n",
      "  <combination_logic>\n",
      "    Execute subquery 1 to get the average order amounts for customers who joined before 2020, broken down by product category. Execute subquery 2 to get the same information for customers who joined after 2020. Compare the results from both subqueries to analyze differences in average order amounts between the two groups for each product category.\n",
      "  </combination_logic>\n",
      "</decomposition>\n",
      "\n",
      "Extracted Subqueries:\n",
      "ID: subquery_1\n",
      "Text: Identify customers who joined before 2020 and calculate the average order amount for each product category for these customers.\n",
      "\n",
      "ID: subquery_2\n",
      "Text: Identify customers who joined after 2020 and calculate the average order amount for each product category for these customers.\n",
      "\n",
      "\n",
      "Decomposition History (2 entries):\n",
      "\n",
      "Entry 1:\n",
      "Query: Find the top 3 product categories by revenue and for each category, show the customer who spent the most on that category\n",
      "Timestamp: 2025-05-21 13:20:58.105777\n",
      "\n",
      "Entry 2:\n",
      "Query: Compare the average order amount between customers who joined before 2020 and those who joined after, broken down by product category\n",
      "Timestamp: 2025-05-21 13:21:01.542099\n"
     ]
    }
   ],
   "source": [
    "# Check what's stored in memory\n",
    "current_decomposition = await memory.get(\"current_decomposition\")\n",
    "print(f\"Current decomposition:\\n{current_decomposition}\\n\")\n",
    "\n",
    "# Get subqueries\n",
    "subqueries_json = await memory.get(\"subqueries\")\n",
    "if subqueries_json:\n",
    "    subqueries = json.loads(subqueries_json)\n",
    "    print(\"Extracted Subqueries:\")\n",
    "    for sq in subqueries:\n",
    "        print(f\"ID: {sq['id']}\")\n",
    "        print(f\"Text: {sq['text']}\\n\")\n",
    "else:\n",
    "    print(\"No subqueries found in memory\")\n",
    "\n",
    "# Get decomposition history\n",
    "decompositions_json = await memory.get(\"previous_decompositions\")\n",
    "if decompositions_json:\n",
    "    decompositions = json.loads(decompositions_json)\n",
    "    print(f\"\\nDecomposition History ({len(decompositions)} entries):\")\n",
    "    for i, entry in enumerate(decompositions):\n",
    "        print(f\"\\nEntry {i+1}:\")\n",
    "        print(f\"Query: {entry['query']}\")\n",
    "        print(f\"Timestamp: {entry['timestamp']}\")\n",
    "else:\n",
    "    print(\"No decomposition history found in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Using Subqueries\n",
    "\n",
    "Let's demonstrate how we might use the decomposed subqueries in a workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subqueries sequentially...\n",
      "\n",
      "Processing subquery_1: Identify customers who joined before 2020 and calculate the average order amount for each product category for these customers.\n",
      "  ‚Üí This would be sent to a SQL generator agent\n",
      "  ‚Üí The SQL would be executed against the database\n",
      "  ‚Üí Results would be stored in memory for combination\n",
      "\n",
      "Processing subquery_2: Identify customers who joined after 2020 and calculate the average order amount for each product category for these customers.\n",
      "  ‚Üí This would be sent to a SQL generator agent\n",
      "  ‚Üí The SQL would be executed against the database\n",
      "  ‚Üí Results would be stored in memory for combination\n",
      "\n",
      "Combination Logic:\n",
      "Execute subquery 1 to get the average order amounts for customers who joined before 2020, broken down by product category. Execute subquery 2 to get the same information for customers who joined after 2020. Compare the results from both subqueries to analyze differences in average order amounts between the two groups for each product category.\n",
      "\n",
      "This logic would be used to combine the results of the subqueries.\n"
     ]
    }
   ],
   "source": [
    "# Get the subqueries\n",
    "subqueries_json = await memory.get(\"subqueries\")\n",
    "if subqueries_json:\n",
    "    subqueries = json.loads(subqueries_json)\n",
    "    print(\"Processing subqueries sequentially...\\n\")\n",
    "    \n",
    "    for sq in subqueries:\n",
    "        print(f\"Processing {sq['id']}: {sq['text']}\")\n",
    "        # In a real workflow, we would send each subquery to a SQL generator agent\n",
    "        # For demonstration, we'll just print what we would do\n",
    "        print(f\"  ‚Üí This would be sent to a SQL generator agent\")\n",
    "        print(f\"  ‚Üí The SQL would be executed against the database\")\n",
    "        print(f\"  ‚Üí Results would be stored in memory for combination\\n\")\n",
    "    \n",
    "    # Get the combination logic\n",
    "    current_decomposition = await memory.get(\"current_decomposition\")\n",
    "    if current_decomposition:\n",
    "        combination_match = re.search(r'<combination_logic>(.+?)</combination_logic>', current_decomposition, re.DOTALL)\n",
    "        if combination_match:\n",
    "            combination_logic = combination_match.group(1).strip()\n",
    "            print(f\"Combination Logic:\\n{combination_logic}\\n\")\n",
    "            print(\"This logic would be used to combine the results of the subqueries.\")\n",
    "else:\n",
    "    print(\"No subqueries available to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrates how the `MemoryAgentTool` allows a decomposer agent to break down complex queries into simpler subqueries while maintaining context through memory. The key features demonstrated include:\n",
    "\n",
    "1. Reading database schema and past decompositions from memory before agent execution\n",
    "2. Parsing and storing decomposition results after agent execution\n",
    "3. Extracting and storing subqueries for easy access by downstream components\n",
    "4. Maintaining a history of decompositions for context\n",
    "\n",
    "This pattern can be extended to multi-agent workflows where the decomposer agent provides structure for other agents to follow, enabling more complex reasoning chains in text-to-SQL applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
