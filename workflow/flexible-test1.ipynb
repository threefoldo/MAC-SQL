{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7eef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Autogen imports\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "# Import from project modules\n",
    "from const import (\n",
    "    SELECTOR_NAME, DECOMPOSER_NAME, REFINER_NAME, SYSTEM_NAME,\n",
    "    selector_template, decompose_template_bird, refiner_template\n",
    ")\n",
    "from schema_manager import SchemaManager\n",
    "from sql_executor import SQLExecutor\n",
    "\n",
    "# Import utilities from utility modules\n",
    "from workflow_utils import (\n",
    "    display_task_info, display_sql_result,\n",
    "    select_schema, generate_sql, refine_sql, process_text_to_sql\n",
    ")\n",
    "from workflow_runners import (\n",
    "    run_text_to_sql_step_by_step, run_complete_text_to_sql, run_all_tests,\n",
    "    DEFAULT_TIMEOUT, MAX_REFINEMENT_ATTEMPTS\n",
    ")\n",
    "\n",
    "# Set constants\n",
    "BIRD_DATA_PATH = \"../data/bird\"\n",
    "BIRD_TABLES_JSON_PATH = os.path.join(BIRD_DATA_PATH, \"dev_tables.json\")\n",
    "DATASET_NAME = \"bird\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7762e8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load json file from ../data/bird/dev_tables.json\n",
      "\n",
      "Loading all database info...\n",
      "Found 11 databases in bird dataset\n"
     ]
    }
   ],
   "source": [
    "schema_manager = SchemaManager(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    tables_json_path=BIRD_TABLES_JSON_PATH,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    lazy=False  # Use lazy loading for performance\n",
    ")\n",
    "\n",
    "# Replace the original SQLExecutor with our patched version\n",
    "sql_executor = SQLExecutor(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    dataset_name=DATASET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c01108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATABASE: california_schools\n",
      "QUERY: List school names of charter schools with an SAT excellence rate over the average.\n",
      "EVIDENCE: Charter schools refers to `Charter School (Y/N)` = 1 in the table frpm; Excellence rate = NumGE1500 / NumTstTakr\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define test cases for BIRD dataset\n",
    "bird_test_cases = [\n",
    "    # Test case 1: Excellence rate calculation (basic join with aggregation)\n",
    "    {\n",
    "        \"db_id\": \"california_schools\",\n",
    "        \"query\": \"List school names of charter schools with an SAT excellence rate over the average.\",\n",
    "        \"evidence\": \"Charter schools refers to `Charter School (Y/N)` = 1 in the table frpm; Excellence rate = NumGE1500 / NumTstTakr\"\n",
    "    },\n",
    "    \n",
    "    # Test case 2: Multi-table query with numeric conditions (multiple joins)\n",
    "    {\n",
    "        \"db_id\": \"game_injury\",\n",
    "        \"query\": \"Show the names of players who have been injured for more than 3 matches in the 2010 season.\",\n",
    "        \"evidence\": \"Season info is in the game table with year 2010; injury severity is measured by the number of matches a player misses.\"\n",
    "    },\n",
    "    \n",
    "    # Test case 3: Complex aggregation with grouping\n",
    "    {\n",
    "        \"db_id\": \"formula_1\",\n",
    "        \"query\": \"What is the name of the driver who has won the most races in rainy conditions?\",\n",
    "        \"evidence\": \"Weather conditions are recorded in the races table; winner information is in the results table.\"\n",
    "    },\n",
    "    \n",
    "    # Test case 4: Temporal query with date handling\n",
    "    {\n",
    "        \"db_id\": \"loan_data\",\n",
    "        \"query\": \"Find the customer with the highest total payment amount for loans taken in the first quarter of 2011.\",\n",
    "        \"evidence\": \"First quarter means January to March (months 1-3); loan dates are stored in ISO format (YYYY-MM-DD).\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Select the test case to run (0-3)\n",
    "test_idx = 0\n",
    "current_test = bird_test_cases[test_idx]\n",
    "\n",
    "# Display test case information\n",
    "display_task_info(current_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b6aad3-5789-4ea6-9a1d-6d4a10a8f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load json file from ../data/bird/dev_tables.json\n",
      "\n",
      "Loading all database info...\n",
      "Found 11 databases in bird dataset\n"
     ]
    }
   ],
   "source": [
    "# Initialize SchemaManager and SQLExecutor\n",
    "schema_manager = SchemaManager(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    tables_json_path=BIRD_TABLES_JSON_PATH,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    lazy=False\n",
    ")\n",
    "\n",
    "sql_executor = SQLExecutor(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n",
    "\n",
    "# Tool implementation for schema selection and management\n",
    "async def get_initial_database_schema(db_id: str) -> str:\n",
    "    \"\"\"Retrieves the full database schema information for a given database.\"\"\"\n",
    "    print(f\"[Tool] Loading schema for database: {db_id}\")\n",
    "    \n",
    "    # Load database information using SchemaManager\n",
    "    if db_id not in schema_manager.db2infos:\n",
    "        schema_manager.db2infos[db_id] = schema_manager._load_single_db_info(db_id)\n",
    "    \n",
    "    # Get database information\n",
    "    db_info = schema_manager.db2dbjsons.get(db_id, {})\n",
    "    if not db_info:\n",
    "        return json.dumps({\"error\": f\"Database '{db_id}' not found\"})\n",
    "    \n",
    "    # Generate full schema description (without pruning)\n",
    "    is_complex = schema_manager._is_complex_schema(db_id)\n",
    "    full_schema_str, full_fk_str, _ = schema_manager.generate_schema_description(\n",
    "        db_id, {}, use_gold_schema=False\n",
    "    )\n",
    "    \n",
    "    # Return schema details\n",
    "    return json.dumps({\n",
    "        \"db_id\": db_id,\n",
    "        \"table_count\": db_info.get('table_count', 0),\n",
    "        \"total_column_count\": db_info.get('total_column_count', 0),\n",
    "        \"avg_column_count\": db_info.get('avg_column_count', 0),\n",
    "        \"is_complex_schema\": is_complex,\n",
    "        \"full_schema_str\": full_schema_str,\n",
    "        \"full_fk_str\": full_fk_str\n",
    "    })\n",
    "\n",
    "async def prune_database_schema(db_id: str, pruning_rules: Dict) -> str:\n",
    "    \"\"\"Applies pruning rules to a database schema.\"\"\"\n",
    "    print(f\"[Tool] Pruning schema for database {db_id}\")\n",
    "    \n",
    "    # Generate pruned schema description\n",
    "    schema_str, fk_str, chosen_schema = schema_manager.generate_schema_description(\n",
    "        db_id, pruning_rules, use_gold_schema=False\n",
    "    )\n",
    "    \n",
    "    # Return pruned schema\n",
    "    return json.dumps({\n",
    "        \"db_id\": db_id,\n",
    "        \"pruning_applied\": True,\n",
    "        \"pruning_rules\": pruning_rules,\n",
    "        \"pruned_schema_str\": schema_str,\n",
    "        \"pruned_fk_str\": fk_str,\n",
    "        \"tables_columns_kept\": chosen_schema\n",
    "    })\n",
    "\n",
    "# Tool implementation for SQL execution\n",
    "async def execute_sql(sql: str, db_id: str) -> str:\n",
    "    \"\"\"Executes a SQL query on the specified database.\"\"\"\n",
    "    print(f\"[Tool] Executing SQL on database {db_id}: {sql[:100]}...\")\n",
    "    \n",
    "    # Execute SQL with timeout protection\n",
    "    result = sql_executor.safe_execute(sql, db_id)\n",
    "    \n",
    "    # Add validation information\n",
    "    is_valid, reason = sql_executor.is_valid_result(result)\n",
    "    result[\"is_valid_result\"] = is_valid\n",
    "    result[\"validation_message\"] = reason\n",
    "    \n",
    "    # Convert to JSON string\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54276cc0-2b29-4710-bb41-57fc563f146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model client\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "# Create Schema Selector Agent\n",
    "selector_agent = AssistantAgent(\n",
    "    name=SELECTOR_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are a Database Schema Selector specialized in analyzing database schemas for text-to-SQL tasks.\n",
    "\n",
    "Your job is to help prune large database schemas to focus on the relevant tables and columns for a given query.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a task with database ID, query, and evidence\n",
    "2. Use the 'get_initial_database_schema' tool to retrieve the full schema\n",
    "3. Analyze the schema complexity and relevance to the query\n",
    "4. For complex schemas, determine which tables and columns are relevant\n",
    "5. Use the 'prune_database_schema' tool to generate a focused schema\n",
    "6. Return the processed schema information for the next agent\n",
    "\n",
    "WHEN ANALYZING THE SCHEMA:\n",
    "- Study the database table structure and relationships\n",
    "- Identify tables directly mentioned or implied in the query\n",
    "- Consider foreign key relationships that might be needed\n",
    "- Follow the pruning guidelines in the following template:\n",
    "{selector_template}\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"pruning_applied\": true/false,\n",
    "  \"schema_str\": \"<schema_description>\",\n",
    "  \"fk_str\": \"<foreign_key_information>\"\n",
    "}}\n",
    "\n",
    "Remember that high-quality schema selection improves the accuracy of SQL generation.\"\"\",\n",
    "    tools=[get_initial_database_schema, prune_database_schema],\n",
    ")\n",
    "\n",
    "# Create Decomposer Agent\n",
    "decomposer_agent = AssistantAgent(\n",
    "    name=DECOMPOSER_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are a Query Decomposer specialized in converting natural language questions into SQL for the BIRD dataset.\n",
    "\n",
    "Your job is to analyze a natural language query and relevant database schema, then generate the appropriate SQL query.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a JSON with db_id, query, evidence, and schema information\n",
    "2. Study the database schema, focusing on tables, columns, and relationships\n",
    "3. For complex queries, break down the problem into logical steps\n",
    "4. Generate a clear, efficient SQL query that answers the question\n",
    "5. Follow the specific query decomposition template for BIRD:\n",
    "{decompose_template_bird}\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "- BIRD queries often require domain knowledge and multiple steps\n",
    "- Carefully use the evidence provided to understand domain-specific concepts\n",
    "- Apply type conversion when comparing numeric data (CAST AS REAL/INT)\n",
    "- Ensure proper handling of NULL values\n",
    "- Use table aliases (T1, T2, etc.) for clarity, especially in JOINs\n",
    "- Always use valid SQLite syntax\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"sql\": \"<generated_sql_query>\",\n",
    "  \"decomposition\": [\n",
    "    \"step1_description\", \n",
    "    \"step2_description\",\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Your goal is to generate SQL that will execute correctly and return the precise information requested.\"\"\",\n",
    "    tools=[],  # SQL generation is the primary LLM task\n",
    ")\n",
    "\n",
    "# Create Refiner Agent\n",
    "refiner_agent = AssistantAgent(\n",
    "    name=REFINER_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are an SQL Refiner specializing in executing and fixing SQL queries for the BIRD dataset.\n",
    "\n",
    "Your job is to test SQL queries against the database, identify errors, and refine them until they execute successfully.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a JSON with db_id, query, evidence, schema, and SQL\n",
    "2. Use the 'execute_sql' tool to run the SQL against the database\n",
    "3. Analyze execution results or errors\n",
    "4. For errors, refine the SQL using the template:\n",
    "{refiner_template}\n",
    "5. For successful execution, validate the results are appropriate for the original query\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "- BIRD databases have specific requirements for valid results:\n",
    "  - Results should not be empty (unless that's the expected answer)\n",
    "  - Results should not contain NULL values without justification\n",
    "  - Results should match the expected types and formats\n",
    "- Focus on SQLite-specific syntax and behaviors\n",
    "- Pay special attention to:\n",
    "  - Table and column name quoting (use backticks)\n",
    "  - Type conversions (CAST AS)\n",
    "  - JOIN conditions\n",
    "  - Subquery structure and aliases\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"original_sql\": \"<original_sql>\",\n",
    "  \"final_sql\": \"<refined_sql>\",\n",
    "  \"status\": \"<EXECUTION_SUCCESSFUL|REFINEMENT_NEEDED|NO_CHANGE_NEEDED>\",\n",
    "  \"execution_result\": \"<execution_result_summary>\",\n",
    "  \"refinement_explanation\": \"<explanation_of_changes>\"\n",
    "}}\n",
    "\n",
    "Your goal is to ensure the SQL query executes successfully and returns relevant results.\"\"\",\n",
    "    tools=[execute_sql],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
