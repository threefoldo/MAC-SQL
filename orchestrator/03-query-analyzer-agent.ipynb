{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analyzer Agent Test\n",
    "\n",
    "This notebook implements and tests the Query Analyzer Agent that understands natural language queries, enriches them using LLM's built-in domain knowledge, and decomposes complex queries into logical parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, List, Optional\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Import unified schemas from our centralized location\n",
    "from schemas import (\n",
    "    # Query Analyzer types\n",
    "    QueryAnalysisInput,\n",
    "    QueryAnalysisOutput,\n",
    "    QueryPlan,\n",
    "    QueryPart,\n",
    "    ExtractedEntitiesAndIntent,\n",
    "    \n",
    "    # Error types\n",
    "    QueryAnalysisError,\n",
    "    \n",
    "    # Database schemas\n",
    "    SCHEMAS,\n",
    "    QUERY_PATTERNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from schema_manager import SchemaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Query Analyzer System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single unified system prompt for query analysis and decomposition\n",
    "QUERY_ANALYZER_SYSTEM_PROMPT = \"\"\"You are an expert Query Analyzer Agent that understands natural language queries about databases.\n",
    "\n",
    "You have two main tasks:\n",
    "\n",
    "1. Query Analysis:\n",
    "   When analyzing queries, return results in this XML format:\n",
    "\n",
    "   <analysis>\n",
    "     <enriched_query>Enriched natural language query with domain knowledge applied</enriched_query>\n",
    "     \n",
    "     <complexity_analysis>\n",
    "       <is_complex>true/false</is_complex>\n",
    "       <complexity_factors>\n",
    "         <factor>Factor description</factor>\n",
    "         ...\n",
    "       </complexity_factors>\n",
    "       <confidence>0.0-1.0</confidence>\n",
    "     </complexity_analysis>\n",
    "     \n",
    "     <entity_extraction>\n",
    "       <metrics>\n",
    "         <metric>metric_name</metric>\n",
    "         ...\n",
    "       </metrics>\n",
    "       <dimensions>\n",
    "         <dimension>dimension_name</dimension>\n",
    "         ...\n",
    "       </dimensions>\n",
    "       <filters>\n",
    "         <filter>\n",
    "           <field>field_name</field>\n",
    "           <operator>operator</operator>\n",
    "           <value>value</value>\n",
    "         </filter>\n",
    "         ...\n",
    "       </filters>\n",
    "       <primary_goal>goal_type</primary_goal>\n",
    "       <confidence>0.0-1.0</confidence>\n",
    "     </entity_extraction>\n",
    "   </analysis>\n",
    "\n",
    "   When enriching queries:\n",
    "   - Understand common synonyms and business terminology from the query context\n",
    "   - Apply logical business rules based on the query intent\n",
    "   - Identify database entities being referenced\n",
    "   - Use your knowledge of database concepts and SQL patterns\n",
    "\n",
    "   For entity extraction, identify:\n",
    "   - Entities: What table and fields are queried\n",
    "   - Metrics: What is being measured (count, sum, average, etc.)\n",
    "   - Dimensions: What groupings or categories are involved\n",
    "   - Filters: What conditions limit the data\n",
    "   - Primary goal: The main purpose (aggregation, comparison, ranking, etc.)\n",
    "\n",
    "   Complex queries include:\n",
    "   - Comparisons with calculated values (e.g., \"above average\")\n",
    "   - Multi-step analyses (e.g., find max then get details)\n",
    "   - Queries combining multiple unrelated questions\n",
    "   - Nested conditions that require intermediate results\n",
    "\n",
    "2. Query Decomposition:\n",
    "   When decomposing complex queries, return results in this XML format:\n",
    "\n",
    "   <decomposition>\n",
    "     <parts>\n",
    "       <part>\n",
    "         <part_id>unique_id</part_id>\n",
    "         <sub_query>Natural language sub-query</sub_query>\n",
    "         <metrics>\n",
    "           <metric>metric_name</metric>\n",
    "           ...\n",
    "         </metrics>\n",
    "         <dimensions>\n",
    "           <dimension>dimension_name</dimension>\n",
    "           ...\n",
    "         </dimensions>\n",
    "         <filters>\n",
    "           <filter>\n",
    "             <field>field_name</field>\n",
    "             <operator>operator</operator>\n",
    "             <value>value</value>\n",
    "           </filter>\n",
    "           ...\n",
    "         </filters>\n",
    "         <primary_goal>goal_type</primary_goal>\n",
    "         <dependencies>\n",
    "           <dependency>part_id</dependency>\n",
    "           ...\n",
    "         </dependencies>\n",
    "       </part>\n",
    "       ...\n",
    "     </parts>\n",
    "     <assembly_instructions>Instructions for combining results</assembly_instructions>\n",
    "   </decomposition>\n",
    "\n",
    "   Each part should be a complete, executable sub-query.\n",
    "   Identify dependencies where one part's results are needed for another.\n",
    "   Provide clear assembly instructions for combining the results.\n",
    "\n",
    "IMPORTANT: Always respond in the appropriate XML format based on the task requested.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import asdict\n",
    "import logging\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QueryAnalyzerAgent:\n",
    "    \"\"\"Agent that analyzes natural language queries and creates execution plans.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"Initialize the Query Analyzer Agent.\"\"\"\n",
    "        self.config = config or {}\n",
    "        self.model = self.config.get('model', 'gpt-4o')\n",
    "        self.model_client = OpenAIChatCompletionClient(model=self.model)\n",
    "        self.agent = self._create_agent()\n",
    "        \n",
    "    def _create_agent(self) -> AssistantAgent:\n",
    "        \"\"\"Create the query analyzer agent.\"\"\"\n",
    "        \n",
    "        # Define deterministic tools that process the agent's responses\n",
    "        async def parse_analysis_xml(xml_response: str) -> str:\n",
    "            \"\"\"Parse XML analysis response from the agent.\"\"\"\n",
    "            try:\n",
    "                xml_match = re.search(r'<analysis>.*?</analysis>', xml_response, re.DOTALL)\n",
    "                if xml_match:\n",
    "                    xml_content = xml_match.group()\n",
    "                    root = ET.fromstring(xml_content)\n",
    "                    \n",
    "                    result = {}\n",
    "                    \n",
    "                    # Extract enriched query\n",
    "                    enriched_elem = root.find('enriched_query')\n",
    "                    if enriched_elem is not None and enriched_elem.text:\n",
    "                        result[\"enriched_query\"] = enriched_elem.text.strip()\n",
    "                    \n",
    "                    # Extract complexity analysis\n",
    "                    complexity_elem = root.find('complexity_analysis')\n",
    "                    if complexity_elem is not None:\n",
    "                        is_complex_elem = complexity_elem.find('is_complex')\n",
    "                        confidence_elem = complexity_elem.find('confidence')\n",
    "                        \n",
    "                        result[\"complexity_analysis\"] = {\n",
    "                            \"is_complex\": is_complex_elem.text.lower() == 'true' if is_complex_elem is not None else False,\n",
    "                            \"confidence\": float(confidence_elem.text) if confidence_elem is not None else 0.5,\n",
    "                            \"complexity_factors\": [f.text for f in complexity_elem.findall('.//factor') if f.text]\n",
    "                        }\n",
    "                    \n",
    "                    # Extract entities\n",
    "                    entity_elem = root.find('entity_extraction')\n",
    "                    if entity_elem is not None:\n",
    "                        metrics = [m.text for m in entity_elem.findall('.//metric') if m.text]\n",
    "                        dimensions = [d.text for d in entity_elem.findall('.//dimension') if d.text]\n",
    "                        \n",
    "                        filters = []\n",
    "                        for filter_elem in entity_elem.findall('.//filter'):\n",
    "                            field_elem = filter_elem.find('field')\n",
    "                            operator_elem = filter_elem.find('operator')\n",
    "                            value_elem = filter_elem.find('value')\n",
    "                            \n",
    "                            if field_elem is not None and operator_elem is not None and value_elem is not None:\n",
    "                                filters.append({\n",
    "                                    \"field\": field_elem.text,\n",
    "                                    \"operator\": operator_elem.text,\n",
    "                                    \"value\": value_elem.text\n",
    "                                })\n",
    "                        \n",
    "                        primary_goal_elem = entity_elem.find('primary_goal')\n",
    "                        confidence_elem = entity_elem.find('confidence')\n",
    "                        \n",
    "                        result[\"entity_extraction\"] = {\n",
    "                            \"metrics\": metrics,\n",
    "                            \"dimensions\": dimensions,\n",
    "                            \"filters\": filters,\n",
    "                            \"primary_goal\": primary_goal_elem.text if primary_goal_elem is not None else \"retrieve\",\n",
    "                            \"confidence\": float(confidence_elem.text) if confidence_elem is not None else 0.5\n",
    "                        }\n",
    "                    \n",
    "                    return json.dumps(result, indent=2)\n",
    "                else:\n",
    "                    raise ValueError(\"No valid XML analysis found in response\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to parse XML analysis: {str(e)}\")\n",
    "                # Return minimal result on failure\n",
    "                default_result = {\n",
    "                    \"enriched_query\": \"\",\n",
    "                    \"complexity_analysis\": {\n",
    "                        \"is_complex\": False,\n",
    "                        \"confidence\": 0.5\n",
    "                    },\n",
    "                    \"entity_extraction\": {\n",
    "                        \"metrics\": [\"data\"],\n",
    "                        \"dimensions\": [],\n",
    "                        \"filters\": [],\n",
    "                        \"primary_goal\": \"retrieve\",\n",
    "                        \"confidence\": 0.5\n",
    "                    }\n",
    "                }\n",
    "                return json.dumps(default_result, indent=2)\n",
    "        \n",
    "        async def parse_decomposition_xml(xml_response: str) -> str:\n",
    "            \"\"\"Parse XML decomposition response from the agent.\"\"\"\n",
    "            try:\n",
    "                xml_match = re.search(r'<decomposition>.*?</decomposition>', xml_response, re.DOTALL)\n",
    "                if xml_match:\n",
    "                    xml_content = xml_match.group()\n",
    "                    root = ET.fromstring(xml_content)\n",
    "                    \n",
    "                    result = {\n",
    "                        \"parts\": [],\n",
    "                        \"assembly_instructions\": \"\"\n",
    "                    }\n",
    "                    \n",
    "                    # Extract parts\n",
    "                    parts_elem = root.find('parts')\n",
    "                    if parts_elem is not None:\n",
    "                        for part_elem in parts_elem.findall('part'):\n",
    "                            part_id_elem = part_elem.find('part_id')\n",
    "                            sub_query_elem = part_elem.find('sub_query')\n",
    "                            \n",
    "                            if part_id_elem is not None and sub_query_elem is not None:\n",
    "                                part_data = {\n",
    "                                    \"part_id\": part_id_elem.text,\n",
    "                                    \"sub_query\": sub_query_elem.text,\n",
    "                                    \"metrics\": [m.text for m in part_elem.findall('.//metric') if m.text],\n",
    "                                    \"dimensions\": [d.text for d in part_elem.findall('.//dimension') if d.text],\n",
    "                                    \"filters\": [],\n",
    "                                    \"dependencies\": [dep.text for dep in part_elem.findall('.//dependency') if dep.text],\n",
    "                                    \"primary_goal\": \"retrieve\"\n",
    "                                }\n",
    "                                \n",
    "                                # Extract filters\n",
    "                                for filter_elem in part_elem.findall('.//filter'):\n",
    "                                    field_elem = filter_elem.find('field')\n",
    "                                    operator_elem = filter_elem.find('operator')\n",
    "                                    value_elem = filter_elem.find('value')\n",
    "                                    \n",
    "                                    if all([field_elem is not None, operator_elem is not None, value_elem is not None]):\n",
    "                                        part_data[\"filters\"].append({\n",
    "                                            \"field\": field_elem.text,\n",
    "                                            \"operator\": operator_elem.text,\n",
    "                                            \"value\": value_elem.text\n",
    "                                        })\n",
    "                                \n",
    "                                primary_goal_elem = part_elem.find('primary_goal')\n",
    "                                if primary_goal_elem is not None:\n",
    "                                    part_data[\"primary_goal\"] = primary_goal_elem.text\n",
    "                                \n",
    "                                result[\"parts\"].append(part_data)\n",
    "                    \n",
    "                    # Extract assembly instructions\n",
    "                    assembly_elem = root.find('assembly_instructions')\n",
    "                    if assembly_elem is not None and assembly_elem.text:\n",
    "                        result[\"assembly_instructions\"] = assembly_elem.text.strip()\n",
    "                    \n",
    "                    return json.dumps(result, indent=2)\n",
    "                else:\n",
    "                    raise ValueError(\"No valid XML decomposition found in response\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to parse XML decomposition: {str(e)}\")\n",
    "                default_result = {\n",
    "                    \"parts\": [],\n",
    "                    \"assembly_instructions\": \"\"\n",
    "                }\n",
    "                return json.dumps(default_result, indent=2)\n",
    "        \n",
    "        # Create the agent with deterministic parsing tools\n",
    "        return AssistantAgent(\n",
    "            name=\"query_analyzer\",\n",
    "            model_client=self.model_client,\n",
    "            tools=[parse_analysis_xml, parse_decomposition_xml],\n",
    "            system_message=QUERY_ANALYZER_SYSTEM_PROMPT,\n",
    "            reflect_on_tool_use=True,\n",
    "            model_client_stream=True,\n",
    "        )\n",
    "    \n",
    "    async def analyze(self, input_data: QueryAnalysisInput) -> QueryAnalysisOutput:\n",
    "        \"\"\"\n",
    "        Main entry point for query analysis.\n",
    "        \n",
    "        Args:\n",
    "            input_data: QueryAnalysisInput containing the query and context\n",
    "            \n",
    "        Returns:\n",
    "            QueryAnalysisOutput with analysis results\n",
    "            \n",
    "        Raises:\n",
    "            QueryAnalysisError: If analysis fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First LLM call - perform initial analysis\n",
    "            analysis_prompt = f\"\"\"\n",
    "            Analyze this query:\n",
    "            Query: \"{input_data.natural_language_query}\"\n",
    "            Database ID: {input_data.database_id}\n",
    "            \n",
    "            Perform the following tasks:\n",
    "            1. Enrich the query with your understanding of database concepts and SQL patterns\n",
    "            2. Analyze query complexity (is it multi-step, has comparisons, etc.)\n",
    "            3. Extract entities and intent (metrics, dimensions, filters, primary goal)\n",
    "            \n",
    "            Use your knowledge of common database terminology, business rules, and SQL patterns to understand the query intent.\n",
    "            \n",
    "            Return analysis in XML format as specified in the system prompt.\n",
    "            \"\"\"\n",
    "            \n",
    "            if input_data.database_schema:\n",
    "                analysis_prompt += f\"\\n\\nDatabase Schema:\\n{input_data.database_schema}\"\n",
    "            \n",
    "            # Run the agent to get XML analysis\n",
    "            analysis_result_str = await self.agent.arun(task=analysis_prompt)\n",
    "            \n",
    "            # Use the parsing tool to extract structured data\n",
    "            parsed_analysis_str = await self.agent.arun(\n",
    "                task=f\"Parse this XML analysis response:\\n{analysis_result_str}\"\n",
    "            )\n",
    "            analysis_result = json.loads(parsed_analysis_str)\n",
    "            \n",
    "            # Extract results\n",
    "            enriched_query = analysis_result.get(\"enriched_query\", input_data.natural_language_query)\n",
    "            complexity_analysis = analysis_result.get(\"complexity_analysis\", {\n",
    "                \"is_complex\": False,\n",
    "                \"confidence\": 0.5\n",
    "            })\n",
    "            main_entities = analysis_result.get(\"entity_extraction\", {})\n",
    "            \n",
    "            # Create ExtractedEntitiesAndIntent object\n",
    "            main_entities_and_intent = ExtractedEntitiesAndIntent(\n",
    "                metrics=main_entities.get(\"metrics\", [\"data\"]),\n",
    "                dimensions=main_entities.get(\"dimensions\", []),\n",
    "                filters=main_entities.get(\"filters\", []),\n",
    "                primary_goal=main_entities.get(\"primary_goal\", \"retrieve\"),\n",
    "                confidence=main_entities.get(\"confidence\", 0.5)\n",
    "            )\n",
    "            \n",
    "            # If complex, second LLM call - decompose the query\n",
    "            if complexity_analysis[\"is_complex\"]:\n",
    "                decomposition_prompt = f\"\"\"\n",
    "                Decompose this complex query:\n",
    "                Query: \"{enriched_query}\"\n",
    "                Database ID: {input_data.database_id}\n",
    "                \n",
    "                Break down the query into logical parts that can be executed sequentially.\n",
    "                Identify dependencies between parts.\n",
    "                Provide assembly instructions for combining results.\n",
    "                \n",
    "                Return decomposition in XML format as specified.\n",
    "                \"\"\"\n",
    "                \n",
    "                if input_data.database_schema:\n",
    "                    decomposition_prompt += f\"\\n\\nDatabase Schema:\\n{input_data.database_schema}\"\n",
    "                \n",
    "                # Run the agent to get XML decomposition\n",
    "                decomposition_result_str = await self.agent.arun(task=decomposition_prompt)\n",
    "                \n",
    "                # Use the parsing tool to extract structured data\n",
    "                parsed_decomposition_str = await self.agent.arun(\n",
    "                    task=f\"Parse this XML decomposition response:\\n{decomposition_result_str}\"\n",
    "                )\n",
    "                decomposition_result = json.loads(parsed_decomposition_str)\n",
    "                \n",
    "                # Convert parsed parts to QueryPart objects\n",
    "                query_parts = []\n",
    "                for part_data in decomposition_result.get(\"parts\", []):\n",
    "                    entities = ExtractedEntitiesAndIntent(\n",
    "                        metrics=part_data.get(\"metrics\", []),\n",
    "                        dimensions=part_data.get(\"dimensions\", []),\n",
    "                        filters=part_data.get(\"filters\", []),\n",
    "                        primary_goal=part_data.get(\"primary_goal\", \"retrieve\"),\n",
    "                        confidence=0.8\n",
    "                    )\n",
    "                    \n",
    "                    query_parts.append(QueryPart(\n",
    "                        part_id=part_data.get(\"part_id\", str(uuid.uuid4())),\n",
    "                        processed_natural_language=part_data.get(\"sub_query\", \"\"),\n",
    "                        extracted_entities_and_intent=entities,\n",
    "                        dependencies=part_data.get(\"dependencies\", []),\n",
    "                        complexity_level=\"complex\"\n",
    "                    ))\n",
    "                \n",
    "                assembly_instructions = decomposition_result.get(\"assembly_instructions\", \"\")\n",
    "                query_type = \"Multi_Part_Query\"\n",
    "            else:\n",
    "                # Single query part\n",
    "                query_parts = [\n",
    "                    QueryPart(\n",
    "                        part_id=str(uuid.uuid4()),\n",
    "                        processed_natural_language=enriched_query,\n",
    "                        extracted_entities_and_intent=main_entities_and_intent,\n",
    "                        dependencies=[],\n",
    "                        complexity_level=\"simple\"\n",
    "                    )\n",
    "                ]\n",
    "                assembly_instructions = None\n",
    "                query_type = \"Single_Query\"\n",
    "            \n",
    "            # Create query plan\n",
    "            query_plan = QueryPlan(\n",
    "                type=query_type,\n",
    "                parts=query_parts,\n",
    "                assembly_instructions_for_multi_part=assembly_instructions\n",
    "            )\n",
    "            \n",
    "            # Create output\n",
    "            output = QueryAnalysisOutput(\n",
    "                query_plan=query_plan,\n",
    "                processed_natural_language=enriched_query,\n",
    "                extracted_entities_and_intent=main_entities_and_intent,\n",
    "                overall_query_understanding_summary=self._create_summary(\n",
    "                    input_data.natural_language_query, \n",
    "                    enriched_query, \n",
    "                    query_plan\n",
    "                ),\n",
    "                confidence_score=complexity_analysis[\"confidence\"]\n",
    "            )\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query analysis failed: {str(e)}\", exc_info=True)\n",
    "            raise QueryAnalysisError(f\"Failed to analyze query: {str(e)}\")\n",
    "    \n",
    "    def _create_summary(self, original: str, enriched: str, plan: QueryPlan) -> str:\n",
    "        \"\"\"Create a summary of the query understanding.\"\"\"\n",
    "        summary_parts = []\n",
    "        \n",
    "        summary_parts.append(f\"Query interpreted as {plan.type}\")\n",
    "        \n",
    "        if original != enriched:\n",
    "            summary_parts.append(\"Query was enriched with domain knowledge\")\n",
    "        \n",
    "        if plan.type == \"Multi_Part_Query\":\n",
    "            summary_parts.append(f\"Decomposed into {len(plan.parts)} parts with dependencies\")\n",
    "        \n",
    "        goals = set(part.extracted_entities_and_intent.primary_goal for part in plan.parts)\n",
    "        summary_parts.append(f\"Primary goals: {', '.join(goals)}\")\n",
    "        \n",
    "        return \". \".join(summary_parts) + \".\"\n",
    "    \n",
    "    async def query(self, task: str) -> None:\n",
    "        \"\"\"Run a query against the analyzer agent.\"\"\"\n",
    "        await Console(self.agent.run_stream(task=task))\n",
    "    \n",
    "    async def close(self) -> None:\n",
    "        \"\"\"Close the model client connection.\"\"\"\n",
    "        await self.model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = {\n",
    "    'model': 'gpt-4o',\n",
    "    'temperature': 0.7,\n",
    "    'max_retry': 3\n",
    "}\n",
    "\n",
    "# Create the query analyzer agent with config\n",
    "query_analyzer = QueryAnalyzerAgent(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Query Analyzer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple query using the new interface\n",
    "async def test_simple_query():\n",
    "    query_input = QueryAnalysisInput(\n",
    "        natural_language_query=\"Show all schools in Alameda county\",\n",
    "        database_id=\"california_schools\",\n",
    "        database_schema=None  # Will use default from SCHEMAS\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await query_analyzer.analyze(query_input)\n",
    "        \n",
    "        print(f\"Query Type: {result.query_plan.type}\")\n",
    "        print(f\"Processed Query: {result.processed_natural_language}\")\n",
    "        print(f\"Confidence: {result.confidence_score}\")\n",
    "        print(f\"Summary: {result.overall_query_understanding_summary}\")\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        print(f\"  Metrics: {result.extracted_entities_and_intent.metrics}\")\n",
    "        print(f\"  Dimensions: {result.extracted_entities_and_intent.dimensions}\")\n",
    "        print(f\"  Filters: {result.extracted_entities_and_intent.filters}\")\n",
    "        print(f\"  Primary Goal: {result.extracted_entities_and_intent.primary_goal}\")\n",
    "        \n",
    "        if result.query_plan.type == \"Single_Query\":\n",
    "            part = result.query_plan.parts[0]\n",
    "            print(f\"\\nQuery Part ID: {part.part_id}\")\n",
    "            print(f\"Natural Language: {part.processed_natural_language}\")\n",
    "            \n",
    "        return result\n",
    "    except QueryAnalysisError as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "result = await test_simple_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complex query involving average comparison\n",
    "async def test_complex_query():\n",
    "    query_input = QueryAnalysisInput(\n",
    "        natural_language_query=\"List school names of charter schools with an SAT excellence rate over the average\",\n",
    "        database_id=\"california_schools\",\n",
    "        database_schema=None\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await query_analyzer.analyze(query_input)\n",
    "        \n",
    "        print(f\"Query Type: {result.query_plan.type}\")\n",
    "        print(f\"Processed Query: {result.processed_natural_language}\")\n",
    "        print(f\"Confidence: {result.confidence_score}\")\n",
    "        print(f\"Summary: {result.overall_query_understanding_summary}\")\n",
    "        \n",
    "        if result.query_plan.type == \"Multi_Part_Query\":\n",
    "            print(f\"\\nQuery decomposed into {len(result.query_plan.parts)} parts:\")\n",
    "            for i, part in enumerate(result.query_plan.parts, 1):\n",
    "                print(f\"\\nPart {i} (ID: {part.part_id}):\")\n",
    "                print(f\"  Natural Language: {part.processed_natural_language}\")\n",
    "                print(f\"  Metrics: {part.extracted_entities_and_intent.metrics}\")\n",
    "                print(f\"  Primary Goal: {part.extracted_entities_and_intent.primary_goal}\")\n",
    "                print(f\"  Dependencies: {part.dependencies}\")\n",
    "            \n",
    "            if result.query_plan.assembly_instructions_for_multi_part:\n",
    "                print(f\"\\nAssembly Instructions: {result.query_plan.assembly_instructions_for_multi_part}\")\n",
    "        \n",
    "        return result\n",
    "    except QueryAnalysisError as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "complex_result = await test_complex_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Complex Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a multi-step query\n",
    "async def test_multi_step_query():\n",
    "    query_input = QueryAnalysisInput(\n",
    "        natural_language_query=\"What is the gender of the youngest client who opened account in the lowest average salary branch?\",\n",
    "        database_id=\"financial\",\n",
    "        database_schema=None\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await query_analyzer.analyze(query_input)\n",
    "        \n",
    "        print(f\"Query Type: {result.query_plan.type}\")\n",
    "        print(f\"Confidence: {result.confidence_score}\")\n",
    "        print(f\"Summary: {result.overall_query_understanding_summary}\")\n",
    "        \n",
    "        print(\"\\nMain Query Analysis:\")\n",
    "        print(f\"  Processed: {result.processed_natural_language}\")\n",
    "        print(f\"  Metrics: {result.extracted_entities_and_intent.metrics}\")\n",
    "        print(f\"  Dimensions: {result.extracted_entities_and_intent.dimensions}\")\n",
    "        \n",
    "        if result.query_plan.type == \"Multi_Part_Query\":\n",
    "            print(f\"\\nDecomposed into {len(result.query_plan.parts)} parts:\")\n",
    "            for i, part in enumerate(result.query_plan.parts, 1):\n",
    "                print(f\"\\nPart {i} (ID: {part.part_id}):\")\n",
    "                print(f\"  Natural Language: {part.processed_natural_language}\")\n",
    "                print(f\"  Primary Goal: {part.extracted_entities_and_intent.primary_goal}\")\n",
    "                if part.dependencies:\n",
    "                    print(f\"  Depends on: {part.dependencies}\")\n",
    "        \n",
    "        return result\n",
    "    except QueryAnalysisError as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "multi_result = await test_multi_step_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Multi-Step Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Built-in Domain Knowledge Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Domain Knowledge Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complex aggregation query\n",
    "async def test_complex_aggregation():\n",
    "    \"\"\"Test a query requiring complex aggregation and grouping.\"\"\"\n",
    "    query_input = QueryAnalysisInput(\n",
    "        natural_language_query=\"For each county with more than 100 schools, show the average SAT math score and the percentage of charter schools\",\n",
    "        database_id=\"california_schools\",\n",
    "        database_schema=None\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await query_analyzer.analyze(query_input)\n",
    "        \n",
    "        print(f\"Query Type: {result.query_plan.type}\")\n",
    "        print(f\"Confidence: {result.confidence_score}\")\n",
    "        print(f\"Summary: {result.overall_query_understanding_summary}\")\n",
    "        \n",
    "        print(\"\\nMain Analysis:\")\n",
    "        entities = result.extracted_entities_and_intent\n",
    "        print(f\"  Metrics: {entities.metrics}\")\n",
    "        print(f\"  Dimensions: {entities.dimensions}\")\n",
    "        print(f\"  Filters: {entities.filters}\")\n",
    "        print(f\"  Primary Goal: {entities.primary_goal}\")\n",
    "        \n",
    "        if result.query_plan.parts:\n",
    "            print(f\"\\nQuery Parts ({len(result.query_plan.parts)}):\")\n",
    "            for i, part in enumerate(result.query_plan.parts, 1):\n",
    "                print(f\"\\nPart {i}:\")\n",
    "                print(f\"  Description: {part.processed_natural_language}\")\n",
    "                part_entities = part.extracted_entities_and_intent\n",
    "                print(f\"  Metrics: {part_entities.metrics}\")\n",
    "                print(f\"  Primary Goal: {part_entities.primary_goal}\")\n",
    "                print(f\"  Complexity: {part.complexity_level}\")\n",
    "        \n",
    "        return result\n",
    "    except QueryAnalysisError as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "aggregation_result = await test_complex_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Complex Aggregation Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Using LLM's Automatic Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Using Custom Domain Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct function call with all parameters\n",
    "async def test_direct_analysis():\n",
    "    \"\"\"Test direct analysis with full control over parameters.\"\"\"\n",
    "    query_input = QueryAnalysisInput(\n",
    "        natural_language_query=\"Find all transactions over $500 from accounts opened in the last year\",\n",
    "        database_id=\"financial\",\n",
    "        database_schema=None,\n",
    "        database_profile=None,  # Will use default\n",
    "        conversation_history=None\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Direct analysis call\n",
    "        result = await query_analyzer.analyze(query_input)\n",
    "        \n",
    "        print(\"Direct Analysis Result:\")\n",
    "        print(f\"Query Type: {result.query_plan.type}\")\n",
    "        print(f\"Confidence: {result.confidence_score}\")\n",
    "        print(f\"Summary: {result.overall_query_understanding_summary}\")\n",
    "        \n",
    "        # Display the analysis results\n",
    "        print(\"\\nMain Query Analysis:\")\n",
    "        main_entities = result.extracted_entities_and_intent\n",
    "        print(f\"  Metrics: {main_entities.metrics}\")\n",
    "        print(f\"  Dimensions: {main_entities.dimensions}\")\n",
    "        print(f\"  Filters: {main_entities.filters}\")\n",
    "        print(f\"  Primary Goal: {main_entities.primary_goal}\")\n",
    "        \n",
    "        print(f\"\\nQuery Parts ({len(result.query_plan.parts)}):\")\n",
    "        for i, part in enumerate(result.query_plan.parts, 1):\n",
    "            print(f\"\\nPart {i}:\")\n",
    "            print(f\"  ID: {part.part_id}\")\n",
    "            print(f\"  Natural Language: {part.processed_natural_language}\")\n",
    "            print(f\"  Primary Goal: {part.extracted_entities_and_intent.primary_goal}\")\n",
    "            if part.dependencies:\n",
    "                print(f\"  Dependencies: {part.dependencies}\")\n",
    "        \n",
    "        return result\n",
    "    except QueryAnalysisError as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "direct_result = await test_direct_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Direct Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test XML parsing capabilities\n",
    "async def test_xml_parsing():\n",
    "    \"\"\"Test the XML parsing mechanism with a complex query that should trigger decomposition.\"\"\"\n",
    "    query_input = QueryAnalysisInput(\n",
    "        natural_language_query=\"Which districts have charter schools with math scores above the district average and also show the percentage of students eligible for free meals?\", \n",
    "        database_id=\"california_schools\",\n",
    "        database_schema=None\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await query_analyzer.analyze(query_input)\n",
    "        \n",
    "        print(\"=== XML Parsing Test Results ===\")\n",
    "        print(f\"Query Type: {result.query_plan.type}\")\n",
    "        print(f\"Confidence: {result.confidence_score}\")\n",
    "        \n",
    "        if result.query_plan.type == \"Multi_Part_Query\":\n",
    "            print(f\"\\nQuery was successfully decomposed into {len(result.query_plan.parts)} parts:\")\n",
    "            \n",
    "            for i, part in enumerate(result.query_plan.parts, 1):\n",
    "                print(f\"\\n--- Part {i} ---\")\n",
    "                print(f\"ID: {part.part_id}\")\n",
    "                print(f\"Query: {part.processed_natural_language}\")\n",
    "                print(f\"Metrics: {part.extracted_entities_and_intent.metrics}\")\n",
    "                print(f\"Dimensions: {part.extracted_entities_and_intent.dimensions}\")\n",
    "                print(f\"Filters: {part.extracted_entities_and_intent.filters}\")\n",
    "                print(f\"Primary Goal: {part.extracted_entities_and_intent.primary_goal}\")\n",
    "                print(f\"Dependencies: {part.dependencies}\")\n",
    "                print(f\"Confidence: {part.extracted_entities_and_intent.confidence}\")\n",
    "            \n",
    "            if result.query_plan.assembly_instructions_for_multi_part:\n",
    "                print(f\"\\nAssembly Instructions:\\n{result.query_plan.assembly_instructions_for_multi_part}\")\n",
    "        else:\n",
    "            print(\"\\nQuery treated as single query:\")\n",
    "            part = result.query_plan.parts[0]\n",
    "            print(f\"Query: {part.processed_natural_language}\")\n",
    "            print(f\"Metrics: {part.extracted_entities_and_intent.metrics}\")\n",
    "            print(f\"Primary Goal: {part.extracted_entities_and_intent.primary_goal}\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the XML parsing test\n",
    "xml_result = await test_xml_parsing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
