{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Sequence, Dict, Any, Tuple, List, Optional\n",
    "\n",
    "# Autogen imports\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
    "\n",
    "# Import from project modules\n",
    "from const import (\n",
    "    SELECTOR_NAME, DECOMPOSER_NAME, REFINER_NAME, SYSTEM_NAME,\n",
    "    selector_template, decompose_template_bird, refiner_template\n",
    ")\n",
    "from schema_manager import SchemaManager\n",
    "from sql_executor import SQLExecutor\n",
    "\n",
    "# --- Constants ---\n",
    "MAX_REFINEMENT_ATTEMPTS = 3  # Maximum number of refinement attempts for SQL\n",
    "BIRD_DATA_PATH = \"../data/bird\"\n",
    "BIRD_TABLES_JSON_PATH = os.path.join(BIRD_DATA_PATH, \"dev_tables.json\")\n",
    "DATASET_NAME = \"bird\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize SchemaManager and SQLExecutor for BIRD dataset\n",
    "schema_manager = SchemaManager(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    tables_json_path=BIRD_TABLES_JSON_PATH,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    lazy=True  # Use lazy loading for better performance\n",
    ")\n",
    "\n",
    "sql_executor = SQLExecutor(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n",
    "\n",
    "# Utility functions for processing agent responses\n",
    "def parse_json(s: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract and parse JSON from agent response text.\"\"\"\n",
    "    try:\n",
    "        # First try parsing the string directly\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON from markdown code blocks\n",
    "        if \"```json\" in s:\n",
    "            json_str = s.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "                \n",
    "        # Try to extract from any code block\n",
    "        if \"```\" in s:\n",
    "            code_block = s.split(\"```\")[1].strip()\n",
    "            try:\n",
    "                return json.loads(code_block)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "                \n",
    "        # Try to find JSON-like content between braces\n",
    "        brace_pattern = r'\\{.*\\}'\n",
    "        match = re.search(brace_pattern, s, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group())\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Return error if all parsing attempts fail\n",
    "        return {\"error\": \"Failed to parse JSON\", \"original_text\": s[:500]}\n",
    "\n",
    "def extract_sql_from_text(text: str) -> str:\n",
    "    \"\"\"Extract SQL query from text or JSON.\"\"\"\n",
    "    # Extract from JSON if it's a dictionary\n",
    "    if isinstance(text, dict):\n",
    "        # Look for the most specific field first\n",
    "        for key in ['refined_sql', 'sql', 'query', 'final_sql']:\n",
    "            if key in text:\n",
    "                return text[key]\n",
    "    \n",
    "    # Extract from SQL code block\n",
    "    sql_pattern = r'```sql\\s*(.*?)\\s*```'\n",
    "    sql_matches = re.findall(sql_pattern, text, re.DOTALL)\n",
    "    if sql_matches:\n",
    "        return sql_matches[0].strip()\n",
    "    \n",
    "    # Look for SQL statements without code blocks\n",
    "    if \"SELECT\" in text.upper():\n",
    "        # Try to extract a whole SQL statement\n",
    "        select_pattern = r'SELECT\\s+.*?(?:;|$)'\n",
    "        select_matches = re.findall(select_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if select_matches:\n",
    "            return select_matches[0].strip()\n",
    "    \n",
    "    # Default return if no SQL found\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tool implementation for schema selection and management\n",
    "async def get_initial_database_schema(db_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the full database schema information for a given database.\n",
    "    \n",
    "    Args:\n",
    "        db_id: The database identifier\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with full schema information\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] Loading schema for database: {db_id}\")\n",
    "    \n",
    "    # Load database information using SchemaManager\n",
    "    if db_id not in schema_manager.db2infos:\n",
    "        schema_manager.db2infos[db_id] = schema_manager._load_single_db_info(db_id)\n",
    "    \n",
    "    # Get database information\n",
    "    db_info = schema_manager.db2dbjsons.get(db_id, {})\n",
    "    if not db_info:\n",
    "        return json.dumps({\"error\": f\"Database '{db_id}' not found\"})\n",
    "    \n",
    "    # Determine if schema is complex enough to need pruning\n",
    "    is_complex = schema_manager._is_complex_schema(db_id)\n",
    "    \n",
    "    # Generate full schema description (without pruning)\n",
    "    full_schema_str, full_fk_str, _ = schema_manager.generate_schema_description(\n",
    "        db_id, {}, use_gold_schema=False\n",
    "    )\n",
    "    \n",
    "    # Return schema details\n",
    "    return json.dumps({\n",
    "        \"db_id\": db_id,\n",
    "        \"table_count\": db_info.get('table_count', 0),\n",
    "        \"total_column_count\": db_info.get('total_column_count', 0),\n",
    "        \"avg_column_count\": db_info.get('avg_column_count', 0),\n",
    "        \"is_complex_schema\": is_complex,\n",
    "        \"full_schema_str\": full_schema_str,\n",
    "        \"full_fk_str\": full_fk_str\n",
    "    })\n",
    "\n",
    "async def prune_database_schema(db_id: str, pruning_rules: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Applies pruning rules to a database schema.\n",
    "    \n",
    "    Args:\n",
    "        db_id: The database identifier\n",
    "        pruning_rules: Dictionary with tables and columns to keep\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with pruned schema\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] Pruning schema for database {db_id}\")\n",
    "    \n",
    "    # Generate pruned schema description\n",
    "    schema_str, fk_str, chosen_schema = schema_manager.generate_schema_description(\n",
    "        db_id, pruning_rules, use_gold_schema=False\n",
    "    )\n",
    "    \n",
    "    # Return pruned schema\n",
    "    return json.dumps({\n",
    "        \"db_id\": db_id,\n",
    "        \"pruning_applied\": True,\n",
    "        \"pruning_rules\": pruning_rules,\n",
    "        \"pruned_schema_str\": schema_str,\n",
    "        \"pruned_fk_str\": fk_str,\n",
    "        \"tables_columns_kept\": chosen_schema\n",
    "    })\n",
    "\n",
    "# Tool implementation for SQL execution\n",
    "async def execute_sql(sql: str, db_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes a SQL query on the specified database.\n",
    "    \n",
    "    Args:\n",
    "        sql: The SQL query to execute\n",
    "        db_id: The database identifier\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with execution results\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] Executing SQL on database {db_id}: {sql[:100]}...\")\n",
    "    \n",
    "    # Execute SQL with timeout protection\n",
    "    result = sql_executor.safe_execute(sql, db_id)\n",
    "    \n",
    "    # Add validation information\n",
    "    is_valid, reason = sql_executor.is_valid_result(result)\n",
    "    result[\"is_valid_result\"] = is_valid\n",
    "    result[\"validation_message\"] = reason\n",
    "    \n",
    "    # Convert to JSON string\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MacSQLTerminationCondition(TextMentionTermination):\n",
    "    \"\"\"Custom termination condition for Text-to-SQL agent workflow.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_refinement_attempts: int = MAX_REFINEMENT_ATTEMPTS):\n",
    "        super().__init__()\n",
    "        self.max_refinement_attempts = max_refinement_attempts\n",
    "        self._refinement_attempts = 0\n",
    "        self._last_sql = None\n",
    "        \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset termination condition state.\"\"\"\n",
    "        self._refinement_attempts = 0\n",
    "        self._last_sql = None\n",
    "        print(\"[Termination] State reset\")\n",
    "        \n",
    "    def __call__(self, messages: Sequence[BaseChatMessage]) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the conversation should terminate.\n",
    "        \n",
    "        Args:\n",
    "            messages: Sequence of messages in the conversation\n",
    "            \n",
    "        Returns:\n",
    "            True if the conversation should terminate, False otherwise\n",
    "        \"\"\"\n",
    "        if not messages:\n",
    "            return False\n",
    "            \n",
    "        # Get the last message\n",
    "        last_message = messages[-1]\n",
    "        \n",
    "        # Get the message content as text\n",
    "        if hasattr(last_message, 'content'):\n",
    "            content = last_message.content\n",
    "            if isinstance(content, dict):\n",
    "                content = str(content)  # Convert dict to string\n",
    "        else:\n",
    "            content = str(last_message)\n",
    "            \n",
    "        # Get message source name\n",
    "        source_name = getattr(last_message.source, 'name', '') if hasattr(last_message, 'source') else ''\n",
    "        \n",
    "        # Try to parse the message content as JSON\n",
    "        try:\n",
    "            data = parse_json(content)\n",
    "        except:\n",
    "            data = {}\n",
    "            \n",
    "        # Handle SQL generation from the decomposer\n",
    "        if source_name == DECOMPOSER_NAME:\n",
    "            sql = extract_sql_from_text(content)\n",
    "            if sql and sql != self._last_sql:\n",
    "                # New SQL detected\n",
    "                self._last_sql = sql\n",
    "                self._refinement_attempts = 0\n",
    "                print(f\"[Termination] New SQL detected from {DECOMPOSER_NAME}. Resetting refinement attempts.\")\n",
    "                return False  # Continue to refiner\n",
    "                \n",
    "        # Handle SQL refinement\n",
    "        if source_name == REFINER_NAME:\n",
    "            # Check for status in the data\n",
    "            status = data.get('status', '')\n",
    "            \n",
    "            # Termination conditions\n",
    "            if status in ['EXECUTION_SUCCESSFUL', 'NO_CHANGE_NEEDED']:\n",
    "                print(f\"[Termination] Success: {status}\")\n",
    "                return True\n",
    "                \n",
    "            # Get the current SQL\n",
    "            sql = extract_sql_from_text(content)\n",
    "            if sql:\n",
    "                # Track attempt for this SQL\n",
    "                if sql != self._last_sql:\n",
    "                    self._last_sql = sql\n",
    "                    self._refinement_attempts = 1\n",
    "                else:\n",
    "                    self._refinement_attempts += 1\n",
    "                    \n",
    "                print(f\"[Termination] Refinement attempt {self._refinement_attempts}/{self.max_refinement_attempts}\")\n",
    "                \n",
    "                # Terminate if max attempts reached\n",
    "                if self._refinement_attempts >= self.max_refinement_attempts:\n",
    "                    print(f\"[Termination] Max refinements ({self.max_refinement_attempts}) reached\")\n",
    "                    return True\n",
    "                    \n",
    "        # Continue if no termination condition met\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
    "    \"\"\"\n",
    "    Determines which agent should receive the next message.\n",
    "    \n",
    "    This function implements the workflow logic:\n",
    "    1. Initial message -> Schema Selector Agent\n",
    "    2. Schema Selector output -> Decomposer Agent\n",
    "    3. Decomposer output -> Refiner Agent\n",
    "    4. Refiner output -> Either Refiner again (for refinement) or terminate\n",
    "    \n",
    "    Args:\n",
    "        messages: Sequence of messages in the conversation\n",
    "        \n",
    "    Returns:\n",
    "        Name of the next agent or None to terminate\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return None\n",
    "        \n",
    "    # Get the last message\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Get content as text\n",
    "    if hasattr(last_message, 'content'):\n",
    "        content = last_message.content\n",
    "        if isinstance(content, dict):\n",
    "            content = str(content)\n",
    "    else:\n",
    "        content = str(last_message)\n",
    "        \n",
    "    # Get source name\n",
    "    source_name = getattr(last_message.source, 'name', '') if hasattr(last_message, 'source') else ''\n",
    "    \n",
    "    # Parse content as JSON\n",
    "    try:\n",
    "        data = parse_json(content)\n",
    "    except:\n",
    "        data = {}\n",
    "        \n",
    "    print(f\"[Selector] Last message from: {source_name}, content type: {type(content)}\")\n",
    "    \n",
    "    # Initial message handling - start with schema selector\n",
    "    if len(messages) == 1:\n",
    "        print(f\"[Selector] Starting new task, selecting {SELECTOR_NAME}\")\n",
    "        return SELECTOR_NAME\n",
    "        \n",
    "    # Workflow transitions\n",
    "    if source_name == SELECTOR_NAME:\n",
    "        # Schema selector -> Decomposer\n",
    "        print(f\"[Selector] Schema processing complete, selecting {DECOMPOSER_NAME}\")\n",
    "        return DECOMPOSER_NAME\n",
    "        \n",
    "    elif source_name == DECOMPOSER_NAME:\n",
    "        # Decomposer -> Refiner\n",
    "        sql = extract_sql_from_text(content)\n",
    "        if sql:\n",
    "            print(f\"[Selector] SQL generated, selecting {REFINER_NAME}\")\n",
    "            return REFINER_NAME\n",
    "        else:\n",
    "            print(f\"[Selector] No SQL found in Decomposer output, terminating\")\n",
    "            return None\n",
    "            \n",
    "    elif source_name == REFINER_NAME:\n",
    "        # Check refiner status\n",
    "        status = data.get('status', '')\n",
    "        \n",
    "        if status in ['EXECUTION_SUCCESSFUL', 'NO_CHANGE_NEEDED']:\n",
    "            # Successful execution or no change needed - terminate\n",
    "            print(f\"[Selector] SQL execution successful or no change needed, terminating\")\n",
    "            return None\n",
    "            \n",
    "        elif status == 'REFINEMENT_NEEDED':\n",
    "            # Continue refinement\n",
    "            print(f\"[Selector] SQL needs refinement, selecting {REFINER_NAME} again\")\n",
    "            return REFINER_NAME\n",
    "            \n",
    "        else:\n",
    "            # Default to continue refinement\n",
    "            print(f\"[Selector] Continuing with {REFINER_NAME} (status: {status})\")\n",
    "            return REFINER_NAME\n",
    "            \n",
    "    # Default to termination for unexpected cases\n",
    "    print(f\"[Selector] No matching rule, terminating\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model client\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "# Create Schema Selector Agent\n",
    "selector_agent = AssistantAgent(\n",
    "    name=SELECTOR_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are a Database Schema Selector specialized in analyzing database schemas for text-to-SQL tasks.\n",
    "\n",
    "Your job is to help prune large database schemas to focus on the relevant tables and columns for a given query.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a task with database ID, query, and evidence\n",
    "2. Use the 'get_initial_database_schema' tool to retrieve the full schema\n",
    "3. Analyze the schema complexity and relevance to the query\n",
    "4. For complex schemas, determine which tables and columns are relevant\n",
    "5. Use the 'prune_database_schema' tool to generate a focused schema\n",
    "6. Return the processed schema information for the next agent\n",
    "\n",
    "WHEN ANALYZING THE SCHEMA:\n",
    "- Study the database table structure and relationships\n",
    "- Identify tables directly mentioned or implied in the query\n",
    "- Consider foreign key relationships that might be needed\n",
    "- Follow the pruning guidelines in the following template:\n",
    "{selector_template}\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"pruning_applied\": true/false,\n",
    "  \"schema_str\": \"<schema_description>\",\n",
    "  \"fk_str\": \"<foreign_key_information>\"\n",
    "}}\n",
    "\n",
    "Remember that high-quality schema selection improves the accuracy of SQL generation.\"\"\",\n",
    "    tools=[get_initial_database_schema, prune_database_schema],\n",
    ")\n",
    "\n",
    "# Create Decomposer Agent\n",
    "decomposer_agent = AssistantAgent(\n",
    "    name=DECOMPOSER_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are a Query Decomposer specialized in converting natural language questions into SQL for the BIRD dataset.\n",
    "\n",
    "Your job is to analyze a natural language query and relevant database schema, then generate the appropriate SQL query.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a JSON with db_id, query, evidence, and schema information\n",
    "2. Study the database schema, focusing on tables, columns, and relationships\n",
    "3. For complex queries, break down the problem into logical steps\n",
    "4. Generate a clear, efficient SQL query that answers the question\n",
    "5. Follow the specific query decomposition template for BIRD:\n",
    "{decompose_template_bird}\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "- BIRD queries often require domain knowledge and multiple steps\n",
    "- Carefully use the evidence provided to understand domain-specific concepts\n",
    "- Apply type conversion when comparing numeric data (CAST AS REAL/INT)\n",
    "- Ensure proper handling of NULL values\n",
    "- Use table aliases (T1, T2, etc.) for clarity, especially in JOINs\n",
    "- Always use valid SQLite syntax\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"sql\": \"<generated_sql_query>\",\n",
    "  \"decomposition\": [\n",
    "    \"step1_description\", \n",
    "    \"step2_description\",\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Your goal is to generate SQL that will execute correctly and return the precise information requested.\"\"\",\n",
    "    tools=[],  # SQL generation is the primary LLM task\n",
    ")\n",
    "\n",
    "# Create Refiner Agent\n",
    "refiner_agent = AssistantAgent(\n",
    "    name=REFINER_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are an SQL Refiner specializing in executing and fixing SQL queries for the BIRD dataset.\n",
    "\n",
    "Your job is to test SQL queries against the database, identify errors, and refine them until they execute successfully.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a JSON with db_id, query, evidence, schema, and SQL\n",
    "2. Use the 'execute_sql' tool to run the SQL against the database\n",
    "3. Analyze execution results or errors\n",
    "4. For errors, refine the SQL using the template:\n",
    "{refiner_template}\n",
    "5. For successful execution, validate the results are appropriate for the original query\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "- BIRD databases have specific requirements for valid results:\n",
    "  - Results should not be empty (unless that's the expected answer)\n",
    "  - Results should not contain NULL values without justification\n",
    "  - Results should match the expected types and formats\n",
    "- Focus on SQLite-specific syntax and behaviors\n",
    "- Pay special attention to:\n",
    "  - Table and column name quoting (use backticks)\n",
    "  - Type conversions (CAST AS)\n",
    "  - JOIN conditions\n",
    "  - Subquery structure and aliases\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"original_sql\": \"<original_sql>\",\n",
    "  \"final_sql\": \"<refined_sql>\",\n",
    "  \"status\": \"<EXECUTION_SUCCESSFUL|REFINEMENT_NEEDED|NO_CHANGE_NEEDED>\",\n",
    "  \"execution_result\": \"<execution_result_summary>\",\n",
    "  \"refinement_explanation\": \"<explanation_of_changes>\"\n",
    "}}\n",
    "\n",
    "Your goal is to ensure the SQL query executes successfully and returns relevant results.\"\"\",\n",
    "    tools=[execute_sql],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up the SelectorGroupChat with the agents\n",
    "termination_condition = MacSQLTerminationCondition(max_refinement_attempts=MAX_REFINEMENT_ATTEMPTS)\n",
    "\n",
    "agents = [selector_agent, decomposer_agent, refiner_agent]\n",
    "\n",
    "# Create the team\n",
    "team = SelectorGroupChat(\n",
    "    agents=agents,\n",
    "    selector_func=selector_func,\n",
    "    termination_condition=termination_condition,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample BIRD dataset query\n",
    "# This is a real query from the BIRD dataset testing \"excellence rate\" calculation\n",
    "bird_task = {\n",
    "    \"db_id\": \"california_schools\",\n",
    "    \"query\": \"List school names of charter schools with an SAT excellence rate over the average.\",\n",
    "    \"evidence\": \"Charter schools refers to `Charter School (Y/N)` = 1 in the table frpm; Excellence rate = NumGE1500 / NumTstTakr\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Execute the task with the agent team\n",
    "# This will process the BIRD query through the schema selection, \n",
    "# SQL generation, and refinement stages\n",
    "await Console(team.run_stream(task=json.dumps(bird_task)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "The example BIRD query demonstrates the full workflow:\n",
    "\n",
    "1. **Schema Selection**: The selector agent analyzes the database schema and identifies the relevant tables (frpm and satscores) for the query about charter schools and SAT scores.\n",
    "\n",
    "2. **SQL Generation**: The decomposer agent uses the BIRD-specific template to handle the domain knowledge about \"excellence rate\" being calculated as NumGE1500/NumTstTakr. It generates SQL that first calculates the average excellence rate of charter schools and then selects schools above that average.\n",
    "\n",
    "3. **SQL Refinement**: The refiner agent executes the SQL against the database and makes any necessary corrections to handle SQLite syntax issues, type conversions, or missing IS NOT NULL checks.\n",
    "\n",
    "The expected SQL solution for this query is:\n",
    "\n",
    "```sql\n",
    "SELECT T2.`sname`\n",
    "FROM frpm AS T1\n",
    "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
    "WHERE T1.`Charter School (Y/N)` = 1 \n",
    "  AND T2.`sname` IS NOT NULL\n",
    "  AND CAST(T2.`NumGE1500` AS REAL) / T2.`NumTstTakr` > (\n",
    "    SELECT AVG(CAST(T4.`NumGE1500` AS REAL) / T4.`NumTstTakr`)\n",
    "    FROM frpm AS T3 \n",
    "    JOIN satscores AS T4 ON T3.`CDSCode` = T4.`cds` \n",
    "    WHERE T3.`Charter School (Y/N)` = 1\n",
    "  )\n",
    "```\n",
    "\n",
    "This query demonstrates several key aspects of BIRD dataset queries:\n",
    "\n",
    "1. Domain knowledge application (charter schools, excellence rate formula)\n",
    "2. Multiple tables with joins\n",
    "3. Type conversion (CAST AS REAL)\n",
    "4. Subquery for comparison with aggregates\n",
    "5. NULL handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}