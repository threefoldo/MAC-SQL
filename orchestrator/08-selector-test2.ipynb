{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813e25ff-20f9-4941-a181-2424190fab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Sequence, Dict, Any, Tuple, List, Optional\n",
    "\n",
    "# Autogen imports\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "# Import from project modules\n",
    "from const import (\n",
    "    SELECTOR_NAME, DECOMPOSER_NAME, REFINER_NAME, SYSTEM_NAME,\n",
    "    selector_template, decompose_template_bird, refiner_template\n",
    ")\n",
    "from schema_manager import SchemaManager\n",
    "from sql_executor import SQLExecutor\n",
    "\n",
    "# Set timeout constants\n",
    "DEFAULT_TIMEOUT = 120  # seconds\n",
    "# --- Constants ---\n",
    "MAX_REFINEMENT_ATTEMPTS = 3  # Maximum number of refinement attempts for SQL\n",
    "BIRD_DATA_PATH = \"../data/bird\"\n",
    "BIRD_TABLES_JSON_PATH = os.path.join(BIRD_DATA_PATH, \"dev_tables.json\")\n",
    "DATASET_NAME = \"bird\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375cd36a-7425-416f-992b-e9154887f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load json file from ../data/bird/dev_tables.json\n",
      "\n",
      "Loading all database info...\n",
      "Found 11 databases in bird dataset\n"
     ]
    }
   ],
   "source": [
    "schema_manager = SchemaManager(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    tables_json_path=BIRD_TABLES_JSON_PATH,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    lazy=False  # Use lazy loading for performance\n",
    ")\n",
    "\n",
    "# Replace the original SQLExecutor with our patched version\n",
    "sql_executor = SQLExecutor(\n",
    "    data_path=BIRD_DATA_PATH,\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n",
    "\n",
    "# Tool implementation for schema selection and management\n",
    "async def get_initial_database_schema(db_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the full database schema information for a given database.\n",
    "    \n",
    "    Args:\n",
    "        db_id: The database identifier\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with full schema information\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] Loading schema for database: {db_id}\")\n",
    "    \n",
    "    # Load database information using SchemaManager\n",
    "    if db_id not in schema_manager.db2infos:\n",
    "        schema_manager.db2infos[db_id] = schema_manager._load_single_db_info(db_id)\n",
    "    \n",
    "    # Get database information\n",
    "    db_info = schema_manager.db2dbjsons.get(db_id, {})\n",
    "    if not db_info:\n",
    "        return json.dumps({\"error\": f\"Database '{db_id}' not found\"})\n",
    "    \n",
    "    # Determine if schema is complex enough to need pruning\n",
    "    is_complex = schema_manager._is_complex_schema(db_id)\n",
    "    \n",
    "    # Generate full schema description (without pruning)\n",
    "    full_schema_str, full_fk_str, _ = schema_manager.generate_schema_description(\n",
    "        db_id, {}, use_gold_schema=False\n",
    "    )\n",
    "    \n",
    "    # Return schema details\n",
    "    return json.dumps({\n",
    "        \"db_id\": db_id,\n",
    "        \"table_count\": db_info.get('table_count', 0),\n",
    "        \"total_column_count\": db_info.get('total_column_count', 0),\n",
    "        \"avg_column_count\": db_info.get('avg_column_count', 0),\n",
    "        \"is_complex_schema\": is_complex,\n",
    "        \"full_schema_str\": full_schema_str,\n",
    "        \"full_fk_str\": full_fk_str\n",
    "    })\n",
    "\n",
    "async def prune_database_schema(db_id: str, pruning_rules: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Applies pruning rules to a database schema.\n",
    "    \n",
    "    Args:\n",
    "        db_id: The database identifier\n",
    "        pruning_rules: Dictionary with tables and columns to keep\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with pruned schema\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] Pruning schema for database {db_id}\")\n",
    "    \n",
    "    # Generate pruned schema description\n",
    "    schema_str, fk_str, chosen_schema = schema_manager.generate_schema_description(\n",
    "        db_id, pruning_rules, use_gold_schema=False\n",
    "    )\n",
    "    \n",
    "    # Return pruned schema\n",
    "    return json.dumps({\n",
    "        \"db_id\": db_id,\n",
    "        \"pruning_applied\": True,\n",
    "        \"pruning_rules\": pruning_rules,\n",
    "        \"pruned_schema_str\": schema_str,\n",
    "        \"pruned_fk_str\": fk_str,\n",
    "        \"tables_columns_kept\": chosen_schema\n",
    "    })\n",
    "\n",
    "# Tool implementation for SQL execution\n",
    "async def execute_sql(sql: str, db_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes a SQL query on the specified database.\n",
    "    \n",
    "    Args:\n",
    "        sql: The SQL query to execute\n",
    "        db_id: The database identifier\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with execution results\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] Executing SQL on database {db_id}: {sql[:100]}...\")\n",
    "    \n",
    "    # Execute SQL with timeout protection\n",
    "    result = sql_executor.safe_execute(sql, db_id)\n",
    "    \n",
    "    # Add validation information\n",
    "    is_valid, reason = sql_executor.is_valid_result(result)\n",
    "    result[\"is_valid_result\"] = is_valid\n",
    "    result[\"validation_message\"] = reason\n",
    "    \n",
    "    # Convert to JSON string\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef7198e-4266-4ca7-a1b8-71090cb9b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper Functions ---\n",
    "def parse_json(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Attempt to parse JSON from text, returning an empty dict if parsing fails.\n",
    "    \n",
    "    Args:\n",
    "        text: Text that might contain JSON\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of parsed JSON or empty dict if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {}\n",
    "            \n",
    "        # Try direct JSON loading first\n",
    "        try:\n",
    "            if text.strip().startswith('{') and text.strip().endswith('}'):\n",
    "                return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "            \n",
    "        # Find JSON-like patterns with regex\n",
    "        json_pattern = r'{.*}'\n",
    "        match = re.search(json_pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            return json.loads(json_str)\n",
    "            \n",
    "        # Try finding JSON in code blocks\n",
    "        code_block_pattern = r'```(?:json)?\\s*(.*?)\\s*```'\n",
    "        blocks = re.findall(code_block_pattern, text, re.DOTALL)\n",
    "        for block in blocks:\n",
    "            if block.strip().startswith('{') and block.strip().endswith('}'):\n",
    "                try:\n",
    "                    return json.loads(block)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cf2a9f-18e5-44af-85f7-d82625983e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract SQL from text using regex and JSON parsing\n",
    "def extract_sql_from_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract SQL query from text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text that might contain SQL\n",
    "        \n",
    "    Returns:\n",
    "        Extracted SQL query or empty string if no SQL found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to extract SQL from JSON\n",
    "        data = parse_json(text)\n",
    "        if 'sql' in data:\n",
    "            return data['sql']\n",
    "        if 'final_sql' in data:\n",
    "            return data['final_sql']\n",
    "            \n",
    "        # Try to extract SQL with regex patterns\n",
    "        sql_patterns = [\n",
    "            r'```sql\\s*(.*?)\\s*```',  # SQL in code blocks\n",
    "            r'```\\s*SELECT.*?```',    # SELECT in generic code blocks\n",
    "            r'SELECT.*?(?:;|$)',      # Simple SELECT statements\n",
    "            r'WITH.*?(?:;|$)',        # WITH queries\n",
    "        ]\n",
    "        \n",
    "        for pattern in sql_patterns:\n",
    "            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "            if matches:\n",
    "                # Clean up the matched SQL\n",
    "                sql = matches[0].strip()\n",
    "                # Remove any trailing backticks or spaces\n",
    "                if sql.endswith('```'):\n",
    "                    sql = sql[:sql.rfind('```')].strip()\n",
    "                return sql\n",
    "        \n",
    "        # If no clear SQL pattern, look for any content between backticks\n",
    "        code_block_pattern = r'```(.*?)```'\n",
    "        code_blocks = re.findall(code_block_pattern, text, re.DOTALL)\n",
    "        for block in code_blocks:\n",
    "            if 'SELECT' in block.upper() or 'WITH' in block.upper():\n",
    "                return block.strip()\n",
    "                \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting SQL: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb48cd7f-e45f-41ab-b74f-cc9cc4daadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model client\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "# Create Schema Selector Agent\n",
    "selector_agent = AssistantAgent(\n",
    "    name=SELECTOR_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are a Database Schema Selector specialized in analyzing database schemas for text-to-SQL tasks.\n",
    "\n",
    "Your job is to help prune large database schemas to focus on the relevant tables and columns for a given query.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a task with database ID, query, and evidence\n",
    "2. Use the 'get_initial_database_schema' tool to retrieve the full schema\n",
    "3. Analyze the schema complexity and relevance to the query\n",
    "4. For complex schemas, determine which tables and columns are relevant\n",
    "5. Use the 'prune_database_schema' tool to generate a focused schema\n",
    "6. Return the processed schema information for the next agent\n",
    "\n",
    "WHEN ANALYZING THE SCHEMA:\n",
    "- Study the database table structure and relationships\n",
    "- Identify tables directly mentioned or implied in the query\n",
    "- Consider foreign key relationships that might be needed\n",
    "- Follow the pruning guidelines in the following template:\n",
    "{selector_template}\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"pruning_applied\": true/false,\n",
    "  \"schema_str\": \"<schema_description>\",\n",
    "  \"fk_str\": \"<foreign_key_information>\"\n",
    "}}\n",
    "\n",
    "Remember that high-quality schema selection improves the accuracy of SQL generation.\"\"\",\n",
    "    tools=[get_initial_database_schema, prune_database_schema],\n",
    ")\n",
    "\n",
    "# Create Decomposer Agent\n",
    "decomposer_agent = AssistantAgent(\n",
    "    name=DECOMPOSER_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are a Query Decomposer specialized in converting natural language questions into SQL for the BIRD dataset.\n",
    "\n",
    "Your job is to analyze a natural language query and relevant database schema, then generate the appropriate SQL query.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a JSON with db_id, query, evidence, and schema information\n",
    "2. Study the database schema, focusing on tables, columns, and relationships\n",
    "3. For complex queries, break down the problem into logical steps\n",
    "4. Generate a clear, efficient SQL query that answers the question\n",
    "5. Follow the specific query decomposition template for BIRD:\n",
    "{decompose_template_bird}\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "- BIRD queries often require domain knowledge and multiple steps\n",
    "- Carefully use the evidence provided to understand domain-specific concepts\n",
    "- Apply type conversion when comparing numeric data (CAST AS REAL/INT)\n",
    "- Ensure proper handling of NULL values\n",
    "- Use table aliases (T1, T2, etc.) for clarity, especially in JOINs\n",
    "- Always use valid SQLite syntax\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"sql\": \"<generated_sql_query>\",\n",
    "  \"decomposition\": [\n",
    "    \"step1_description\", \n",
    "    \"step2_description\",\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Your goal is to generate SQL that will execute correctly and return the precise information requested.\"\"\",\n",
    "    tools=[],  # SQL generation is the primary LLM task\n",
    ")\n",
    "\n",
    "# Create Refiner Agent\n",
    "refiner_agent = AssistantAgent(\n",
    "    name=REFINER_NAME,\n",
    "    model_client=model_client,\n",
    "    system_message=f\"\"\"You are an SQL Refiner specializing in executing and fixing SQL queries for the BIRD dataset.\n",
    "\n",
    "Your job is to test SQL queries against the database, identify errors, and refine them until they execute successfully.\n",
    "\n",
    "TASK OVERVIEW:\n",
    "1. You will receive a JSON with db_id, query, evidence, schema, and SQL\n",
    "2. Use the 'execute_sql' tool to run the SQL against the database\n",
    "3. Analyze execution results or errors\n",
    "4. For errors, refine the SQL using the template:\n",
    "{refiner_template}\n",
    "5. For successful execution, validate the results are appropriate for the original query\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "- BIRD databases have specific requirements for valid results:\n",
    "  - Results should not be empty (unless that's the expected answer)\n",
    "  - Results should not contain NULL values without justification\n",
    "  - Results should match the expected types and formats\n",
    "- Focus on SQLite-specific syntax and behaviors\n",
    "- Pay special attention to:\n",
    "  - Table and column name quoting (use backticks)\n",
    "  - Type conversions (CAST AS)\n",
    "  - JOIN conditions\n",
    "  - Subquery structure and aliases\n",
    "\n",
    "FORMAT YOUR FINAL RESPONSE AS JSON:\n",
    "{{\n",
    "  \"db_id\": \"<database_id>\",\n",
    "  \"query\": \"<natural_language_query>\",\n",
    "  \"evidence\": \"<any_evidence_provided>\",\n",
    "  \"original_sql\": \"<original_sql>\",\n",
    "  \"final_sql\": \"<refined_sql>\",\n",
    "  \"status\": \"<EXECUTION_SUCCESSFUL|REFINEMENT_NEEDED|NO_CHANGE_NEEDED>\",\n",
    "  \"execution_result\": \"<execution_result_summary>\",\n",
    "  \"refinement_explanation\": \"<explanation_of_changes>\"\n",
    "}}\n",
    "\n",
    "Your goal is to ensure the SQL query executes successfully and returns relevant results.\"\"\",\n",
    "    tools=[execute_sql],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101bc4bd-66f2-4e76-9eeb-24c8ea06ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Schema Selection\n",
    "async def select_schema(task_json: str, timeout: int = DEFAULT_TIMEOUT):\n",
    "    \"\"\"\n",
    "    First workflow step: Schema selection with selector_agent\n",
    "    \n",
    "    Args:\n",
    "        task_json: JSON string containing the task information\n",
    "        timeout: Maximum time to wait for response in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (task dict, selector response content)\n",
    "    \"\"\"\n",
    "    # Create cancellation token with timeout\n",
    "    cancellation_token = CancellationToken()\n",
    "    \n",
    "    # Parse task\n",
    "    try:\n",
    "        task = json.loads(task_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[Step 1] Error: Invalid JSON input: {str(e)}\")\n",
    "        raise ValueError(f\"Invalid task JSON: {str(e)}\")\n",
    "        \n",
    "    db_id = task.get('db_id', '')\n",
    "    query = task.get('query', '')\n",
    "    evidence = task.get('evidence', '')\n",
    "    \n",
    "    if not db_id or not query:\n",
    "        raise ValueError(\"Task must contain db_id and query\")\n",
    "        \n",
    "    print(f\"[Step 1] Starting schema selection for database '{db_id}'\")\n",
    "    print(f\"[Step 1] Query: {query}\")\n",
    "    print(f\"[Step 1] Evidence: {evidence}\")\n",
    "    \n",
    "    # Ensure task has the expected format for selector_agent\n",
    "    task_content = task_json\n",
    "    # If task_json isn't properly formatted, create a well-structured message\n",
    "    if not '\"db_id\"' in task_json or not task_json.strip().startswith('{'):\n",
    "        task_content = json.dumps({\n",
    "            \"db_id\": db_id,\n",
    "            \"query\": query,\n",
    "            \"evidence\": evidence\n",
    "        })\n",
    "    \n",
    "    # Create proper message object\n",
    "    user_message = TextMessage(content=task_content, source=\"user\")\n",
    "    \n",
    "    # Execute schema selection with timeout\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"[Step 1] Requesting schema selection (timeout: {timeout}s)\")\n",
    "        selector_response = await selector_agent.on_messages([user_message], cancellation_token)\n",
    "        selector_content = selector_response.chat_message.content.strip()\n",
    "        \n",
    "        # Verify selector output contains schema information\n",
    "        if \"<database_schema>\" not in selector_content and \"schema_str\" not in selector_content:\n",
    "            print(f\"[Step 1] Warning: Selector response may not contain valid schema information\")\n",
    "            \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"[Step 1] Schema selected successfully (took {elapsed:.1f}s)\")\n",
    "        return task, selector_content\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"[Step 1] Error: Schema selection timed out after {timeout}s\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"[Step 1] Error in schema selection: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545a71f8-03e4-4f69-80ef-dce54826d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: SQL Generation\n",
    "async def generate_sql(selector_content: str, task: dict, timeout: int = DEFAULT_TIMEOUT):\n",
    "    \"\"\"\n",
    "    Second workflow step: SQL generation with decomposer_agent\n",
    "    \n",
    "    Args:\n",
    "        selector_content: Output from the schema selection step\n",
    "        task: Task dictionary containing query information\n",
    "        timeout: Maximum time to wait for response in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (decomposer content, extracted SQL)\n",
    "    \"\"\"\n",
    "    # Create cancellation token with timeout\n",
    "    cancellation_token = CancellationToken()\n",
    "    \n",
    "    print(f\"\\n[Step 2] Starting SQL generation\")\n",
    "    print(f\"[Step 2] Query: {task.get('query', '')}\")\n",
    "    \n",
    "    # Extract schema from selector content if possible\n",
    "    schema_str = \"\"\n",
    "    try:\n",
    "        # Try to extract schema from JSON format\n",
    "        data = parse_json(selector_content)\n",
    "        if 'schema_str' in data:\n",
    "            schema_str = data['schema_str']\n",
    "            print(f\"[Step 2] Found schema information in JSON format\")\n",
    "    except Exception:\n",
    "        # If JSON parsing fails, try to extract schema directly\n",
    "        schema_match = re.search(r'<database_schema>.*?</database_schema>', selector_content, re.DOTALL)\n",
    "        if schema_match:\n",
    "            schema_str = schema_match.group()\n",
    "            print(f\"[Step 2] Found schema information in XML format\")\n",
    "    \n",
    "    if not schema_str:\n",
    "        print(f\"[Step 2] Warning: Could not extract schema from selector output\")\n",
    "    \n",
    "    # Ensure selector_content is properly formatted for the decomposer agent\n",
    "    if not schema_str and \"<database_schema>\" not in selector_content:\n",
    "        print(f\"[Step 2] Warning: Reformatting selector content to ensure schema is included\")\n",
    "        # Try to reformat it as a proper JSON\n",
    "        try:\n",
    "            data = parse_json(selector_content)\n",
    "            if not 'schema_str' in data:\n",
    "                # If we parsed JSON but it doesn't have schema_str, try to extract it directly\n",
    "                schema_match = re.search(r'<database_schema>.*?</database_schema>', selector_content, re.DOTALL)\n",
    "                if schema_match:\n",
    "                    data['schema_str'] = schema_match.group()\n",
    "                    selector_content = json.dumps(data)\n",
    "        except Exception:\n",
    "            # If all fails, just use the original content\n",
    "            pass\n",
    "    \n",
    "    # Create proper message object\n",
    "    selector_message = TextMessage(content=selector_content, source=SELECTOR_NAME)\n",
    "    \n",
    "    # Execute SQL generation with timeout\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"[Step 2] Requesting SQL generation (timeout: {timeout}s)\")\n",
    "        decomposer_response = await decomposer_agent.on_messages([selector_message], cancellation_token)\n",
    "        decomposer_content = decomposer_response.chat_message.content.strip()\n",
    "        \n",
    "        # Extract SQL from decomposer output\n",
    "        sql = extract_sql_from_text(decomposer_content)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"[Step 2] SQL generation completed (took {elapsed:.1f}s)\")\n",
    "        \n",
    "        if not sql:\n",
    "            print(f\"[Step 2] Warning: No SQL found in decomposer output\")\n",
    "        else:\n",
    "            print(f\"[Step 2] SQL generated: {sql[:100]}...\")\n",
    "            \n",
    "        return decomposer_content, sql\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"[Step 2] Error: SQL generation timed out after {timeout}s\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"[Step 2] Error in SQL generation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18b93e6-07a5-4d50-b4a9-7eeaa6ca2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: SQL Refinement\n",
    "async def refine_sql(decomposer_content: str, sql: str, task: dict, \n",
    "                    max_refinement_attempts: int = MAX_REFINEMENT_ATTEMPTS,\n",
    "                    timeout: int = DEFAULT_TIMEOUT):\n",
    "    \"\"\"\n",
    "    Third workflow step: SQL refinement with refiner_agent\n",
    "    \n",
    "    Args:\n",
    "        decomposer_content: Output from the SQL generation step\n",
    "        sql: Initial SQL query extracted from decomposer_content\n",
    "        task: Task dictionary containing query information\n",
    "        max_refinement_attempts: Maximum number of refinement iterations\n",
    "        timeout: Maximum time to wait for each response in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Refinement results dictionary\n",
    "    \"\"\"\n",
    "    # Handle case where no SQL was generated\n",
    "    if not sql:\n",
    "        print(f\"[Step 3] No SQL to refine, skipping refinement step\")\n",
    "        return {\n",
    "            \"db_id\": task.get('db_id', ''),\n",
    "            \"query\": task.get('query', ''),\n",
    "            \"evidence\": task.get('evidence', ''),\n",
    "            \"final_output\": decomposer_content,\n",
    "            \"error\": \"No SQL generated\",\n",
    "            \"refinement_attempts\": 0,\n",
    "            \"status\": \"ERROR_NO_SQL\"\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n[Step 3] Starting SQL refinement\")\n",
    "    print(f\"[Step 3] Initial SQL: {sql[:100]}...\")\n",
    "    \n",
    "    # Extract schema information from decomposer content or selector content\n",
    "    schema_info = \"\"\n",
    "    try:\n",
    "        # Look for schema in decomposer content\n",
    "        schema_match = re.search(r'<database_schema>.*?</database_schema>', decomposer_content, re.DOTALL)\n",
    "        if schema_match:\n",
    "            schema_info = schema_match.group()\n",
    "        \n",
    "        # If not found, try parsing JSON\n",
    "        if not schema_info:\n",
    "            data = parse_json(decomposer_content)\n",
    "            if 'schema_str' in data:\n",
    "                schema_info = data['schema_str']\n",
    "    except Exception as e:\n",
    "        print(f\"[Step 3] Warning: Could not extract schema information: {str(e)}\")\n",
    "    \n",
    "    # Create a structured input for the refiner with all necessary context\n",
    "    refiner_input = {\n",
    "        \"db_id\": task.get('db_id', ''),\n",
    "        \"query\": task.get('query', ''),\n",
    "        \"evidence\": task.get('evidence', ''),\n",
    "        \"sql\": sql,\n",
    "        \"schema_info\": schema_info,  # Include schema information when available\n",
    "        \"instructions\": \"Please execute this SQL against the database and return the results in a structured JSON format with the fields: status, final_sql, and execution_result.\"\n",
    "    }\n",
    "    \n",
    "    refiner_content = json.dumps(refiner_input)\n",
    "    last_sql = sql\n",
    "    refinement_attempts = 0\n",
    "    \n",
    "    while refinement_attempts < max_refinement_attempts:\n",
    "        attempt_number = refinement_attempts + 1\n",
    "        print(f\"[Step 3] Starting refinement attempt {attempt_number}/{max_refinement_attempts}\")\n",
    "        \n",
    "        # Create cancellation token with timeout\n",
    "        cancellation_token = CancellationToken()\n",
    "        \n",
    "        # Create proper message object with the appropriate source\n",
    "        message = TextMessage(\n",
    "            content=refiner_content, \n",
    "            source=DECOMPOSER_NAME if refinement_attempts == 0 else REFINER_NAME\n",
    "        )\n",
    "        \n",
    "        # Execute refinement with timeout\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            print(f\"[Step 3] Requesting refinement (timeout: {timeout}s)\")\n",
    "            refiner_response = await refiner_agent.on_messages([message], cancellation_token)\n",
    "            refiner_content = refiner_response.chat_message.content.strip()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"[Step 3] Refinement response received (took {elapsed:.1f}s)\")\n",
    "            \n",
    "            # Parse refiner output (with fallback)\n",
    "            data = {}\n",
    "            try:\n",
    "                data = parse_json(refiner_content)\n",
    "            except Exception as e:\n",
    "                print(f\"[Step 3] Warning: Could not parse refiner response as JSON: {str(e)}\")\n",
    "                # Create a simple data structure if parsing failed\n",
    "                data = {\"error\": str(e)}\n",
    "            \n",
    "            status = data.get('status', '')\n",
    "            print(f\"[Step 3] Refinement attempt {attempt_number}, status: {status}\")\n",
    "            \n",
    "            # Check for termination conditions\n",
    "            if status in ['EXECUTION_SUCCESSFUL', 'NO_CHANGE_NEEDED', 'EXECUTION_CONFIRMED']:\n",
    "                print(f\"[Step 3] SQL execution successful: {status}\")\n",
    "                break\n",
    "                \n",
    "            # Extract the refined SQL\n",
    "            new_sql = extract_sql_from_text(refiner_content)\n",
    "            \n",
    "            if new_sql:\n",
    "                if new_sql == last_sql:\n",
    "                    # SQL didn't change\n",
    "                    print(f\"[Step 3] SQL unchanged in attempt {attempt_number}\")\n",
    "                else:\n",
    "                    # SQL changed\n",
    "                    print(f\"[Step 3] New SQL detected: {new_sql[:100]}...\")\n",
    "                    last_sql = new_sql\n",
    "            else:\n",
    "                # No SQL found in response, force a better input format for next attempt\n",
    "                print(f\"[Step 3] No SQL found in refinement output, attempt {attempt_number}\")\n",
    "                \n",
    "                refiner_input = {\n",
    "                    \"db_id\": task.get('db_id', ''),\n",
    "                    \"query\": task.get('query', ''),\n",
    "                    \"evidence\": task.get('evidence', ''),\n",
    "                    \"sql\": last_sql,\n",
    "                    \"schema_info\": schema_info,\n",
    "                    \"refiner_instructions\": \"Please execute this SQL query against the database and provide the following in your response as a JSON object:\\n - status: 'EXECUTION_SUCCESSFUL', 'REFINEMENT_NEEDED', or 'NO_CHANGE_NEEDED'\\n - final_sql: The final SQL query (same as input if no changes needed)\\n - execution_result: The result of executing the query\",\n",
    "                    \"attempt\": attempt_number\n",
    "                }\n",
    "                refiner_content = json.dumps(refiner_input)\n",
    "                \n",
    "            # Increment refinement attempts and check if max reached\n",
    "            refinement_attempts += 1\n",
    "            if refinement_attempts >= max_refinement_attempts:\n",
    "                print(f\"[Step 3] Max refinements ({max_refinement_attempts}) reached\")\n",
    "                break\n",
    "                \n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"[Step 3] Error: Refinement attempt {attempt_number} timed out after {timeout}s\")\n",
    "            refinement_attempts += 1\n",
    "            \n",
    "            # Try a simplified input for next attempt\n",
    "            refiner_input = {\n",
    "                \"db_id\": task.get('db_id', ''),\n",
    "                \"query\": task.get('query', ''),\n",
    "                \"sql\": last_sql,\n",
    "                \"timeout_error\": f\"Previous attempt timed out after {timeout}s\",\n",
    "                \"instructions\": \"Please execute this SQL against the database and return the results.\"\n",
    "            }\n",
    "            refiner_content = json.dumps(refiner_input)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[Step 3] Error in refinement attempt {attempt_number}: {str(e)}\")\n",
    "            refinement_attempts += 1\n",
    "            \n",
    "            # Try a simplified input for next attempt\n",
    "            refiner_input = {\n",
    "                \"db_id\": task.get('db_id', ''),\n",
    "                \"query\": task.get('query', ''),\n",
    "                \"sql\": last_sql,\n",
    "                \"error\": str(e),\n",
    "                \"instructions\": \"Please execute this SQL against the database and return the results.\"\n",
    "            }\n",
    "            refiner_content = json.dumps(refiner_input)\n",
    "    \n",
    "    # Extract final SQL and status from refiner output \n",
    "    final_sql = extract_sql_from_text(refiner_content) or last_sql\n",
    "    \n",
    "    # Prepare final result with fallback\n",
    "    try:\n",
    "        data = parse_json(refiner_content)\n",
    "        status = data.get('status', 'UNKNOWN')\n",
    "        \n",
    "        # If data doesn't have a final_sql field but we extracted one, add it\n",
    "        if final_sql and not data.get('final_sql'):\n",
    "            data['final_sql'] = final_sql\n",
    "            refiner_content = json.dumps(data)\n",
    "    except Exception as e:\n",
    "        print(f\"[Step 3] Error parsing final result: {str(e)}\")\n",
    "        # Create a minimal result if parsing failed\n",
    "        data = {\n",
    "            \"status\": \"ERROR_PARSING_RESULT\",\n",
    "            \"final_sql\": final_sql,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        refiner_content = json.dumps(data)\n",
    "        status = \"ERROR_PARSING_RESULT\"\n",
    "    \n",
    "    print(f\"[Step 3] Refinement complete - Final status: {status}\")\n",
    "    if final_sql:\n",
    "        print(f\"[Step 3] Final SQL: {final_sql[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"db_id\": task.get('db_id', ''),\n",
    "        \"query\": task.get('query', ''),\n",
    "        \"evidence\": task.get('evidence', ''),\n",
    "        \"final_output\": refiner_content,\n",
    "        \"refinement_attempts\": refinement_attempts,\n",
    "        \"status\": status,\n",
    "        \"final_sql\": final_sql\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b6a5a1-f249-4582-97aa-2ee9f12200be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: SQL Refinement\n",
    "async def refine_sql(decomposer_content: str, sql: str, task: dict, max_refinement_attempts: int = MAX_REFINEMENT_ATTEMPTS):\n",
    "    \"\"\"\n",
    "    Third workflow step: SQL refinement with refiner_agent\n",
    "    \n",
    "    Args:\n",
    "        decomposer_content: Output from the SQL generation step\n",
    "        sql: Initial SQL query extracted from decomposer_content\n",
    "        task: Task dictionary containing query information\n",
    "        max_refinement_attempts: Maximum number of refinement iterations\n",
    "        \n",
    "    Returns:\n",
    "        Refinement results dictionary\n",
    "    \"\"\"\n",
    "    # Create cancellation token\n",
    "    cancellation_token = CancellationToken()\n",
    "    \n",
    "    if not sql:\n",
    "        print(f\"[Step 3] No SQL to refine, skipping refinement step\")\n",
    "        return {\n",
    "            \"db_id\": task.get('db_id', ''),\n",
    "            \"query\": task.get('query', ''),\n",
    "            \"evidence\": task.get('evidence', ''),\n",
    "            \"final_output\": decomposer_content,\n",
    "            \"error\": \"No SQL generated\",\n",
    "            \"refinement_attempts\": 0\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n[Step 3] Starting SQL refinement\")\n",
    "    print(f\"[Step 3] Initial SQL: {sql[:100]}...\")\n",
    "    \n",
    "    # Extract schema information from decomposer content or selector content\n",
    "    schema_info = \"\"\n",
    "    try:\n",
    "        # Look for schema in decomposer content\n",
    "        schema_match = re.search(r'<database_schema>.*?</database_schema>', decomposer_content, re.DOTALL)\n",
    "        if schema_match:\n",
    "            schema_info = schema_match.group()\n",
    "        \n",
    "        # If not found, try parsing JSON\n",
    "        if not schema_info:\n",
    "            data = parse_json(decomposer_content)\n",
    "            if 'schema_str' in data:\n",
    "                schema_info = data['schema_str']\n",
    "    except Exception as e:\n",
    "        print(f\"[Step 3] Warning: Could not extract schema information: {str(e)}\")\n",
    "    \n",
    "    # Create a structured input for the refiner with all necessary context\n",
    "    refiner_input = {\n",
    "        \"db_id\": task.get('db_id', ''),\n",
    "        \"query\": task.get('query', ''),\n",
    "        \"evidence\": task.get('evidence', ''),\n",
    "        \"sql\": sql,\n",
    "        \"schema_info\": schema_info  # Include schema information when available\n",
    "    }\n",
    "    \n",
    "    refiner_content = json.dumps(refiner_input)\n",
    "    last_sql = sql\n",
    "    refinement_attempts = 0\n",
    "    \n",
    "    while refinement_attempts < max_refinement_attempts:\n",
    "        print(f\"[Step 3] Starting refinement attempt {refinement_attempts + 1}/{max_refinement_attempts}\")\n",
    "        \n",
    "        # Create proper message object with the appropriate source\n",
    "        message = TextMessage(\n",
    "            content=refiner_content, \n",
    "            source=DECOMPOSER_NAME if refinement_attempts == 0 else REFINER_NAME\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Execute refinement\n",
    "            refiner_response = await refiner_agent.on_messages([message], cancellation_token)\n",
    "            refiner_content = refiner_response.chat_message.content.strip()\n",
    "            \n",
    "            # Parse refiner output\n",
    "            data = parse_json(refiner_content)\n",
    "            status = data.get('status', '')\n",
    "            print(f\"[Step 3] Refinement attempt {refinement_attempts + 1}, status: {status}\")\n",
    "            \n",
    "            # Check for termination conditions\n",
    "            if status in ['EXECUTION_SUCCESSFUL', 'NO_CHANGE_NEEDED', 'EXECUTION_CONFIRMED']:\n",
    "                print(f\"[Step 3] SQL execution successful: {status}\")\n",
    "                break\n",
    "                \n",
    "            # Extract the refined SQL\n",
    "            new_sql = extract_sql_from_text(refiner_content)\n",
    "            \n",
    "            if new_sql:\n",
    "                if new_sql == last_sql:\n",
    "                    # SQL didn't change\n",
    "                    print(f\"[Step 3] SQL unchanged in attempt {refinement_attempts + 1}\")\n",
    "                else:\n",
    "                    # SQL changed\n",
    "                    print(f\"[Step 3] New SQL detected: {new_sql[:100]}...\")\n",
    "                    last_sql = new_sql\n",
    "            else:\n",
    "                # No SQL found in response\n",
    "                print(f\"[Step 3] No SQL found in refinement output, attempt {refinement_attempts + 1}\")\n",
    "                \n",
    "                # Create a more structured input for next attempt\n",
    "                refiner_input = {\n",
    "                    \"db_id\": task.get('db_id', ''),\n",
    "                    \"query\": task.get('query', ''),\n",
    "                    \"evidence\": task.get('evidence', ''),\n",
    "                    \"sql\": last_sql,\n",
    "                    \"schema_info\": schema_info,\n",
    "                    \"refiner_instructions\": \"Please execute this SQL and provide a status and final_sql in your response JSON.\"\n",
    "                }\n",
    "                refiner_content = json.dumps(refiner_input)\n",
    "                \n",
    "            # Increment refinement attempts and check if max reached\n",
    "            refinement_attempts += 1\n",
    "            if refinement_attempts >= max_refinement_attempts:\n",
    "                print(f\"[Step 3] Max refinements ({max_refinement_attempts}) reached\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[Step 3] Error in refinement attempt {refinement_attempts + 1}: {str(e)}\")\n",
    "            refinement_attempts += 1\n",
    "            # In case of error, simplify the input for the next attempt\n",
    "            refiner_input = {\n",
    "                \"db_id\": task.get('db_id', ''),\n",
    "                \"query\": task.get('query', ''),\n",
    "                \"sql\": last_sql,\n",
    "                \"error\": str(e),\n",
    "                \"instructions\": \"Please execute this SQL against the database and return the results in JSON format.\"\n",
    "            }\n",
    "            refiner_content = json.dumps(refiner_input)\n",
    "    \n",
    "    # Extract final SQL and status from refiner output \n",
    "    final_sql = extract_sql_from_text(refiner_content) or last_sql\n",
    "    \n",
    "    try:\n",
    "        data = parse_json(refiner_content)\n",
    "        status = data.get('status', 'UNKNOWN')\n",
    "        \n",
    "        # If data doesn't have a final_sql field but we extracted one, add it to create a proper result\n",
    "        if final_sql and not data.get('final_sql'):\n",
    "            data['final_sql'] = final_sql\n",
    "            refiner_content = json.dumps(data)\n",
    "    except Exception as e:\n",
    "        print(f\"[Step 3] Error parsing final result: {str(e)}\")\n",
    "        # Create a minimal result if parsing failed\n",
    "        data = {\n",
    "            \"status\": \"ERROR_PARSING_RESULT\",\n",
    "            \"final_sql\": final_sql,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        refiner_content = json.dumps(data)\n",
    "        status = \"ERROR_PARSING_RESULT\"\n",
    "    \n",
    "    print(f\"[Step 3] Refinement complete - Final status: {status}\")\n",
    "    if final_sql:\n",
    "        print(f\"[Step 3] Final SQL: {final_sql[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"db_id\": task.get('db_id', ''),\n",
    "        \"query\": task.get('query', ''),\n",
    "        \"evidence\": task.get('evidence', ''),\n",
    "        \"final_output\": refiner_content,\n",
    "        \"refinement_attempts\": refinement_attempts,\n",
    "        \"status\": status,\n",
    "        \"final_sql\": final_sql\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ecd7b61-21a4-43a4-91e9-f33b93c1dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected test case 1:\n",
      "Database: california_schools\n",
      "Query: List school names of charter schools with an SAT excellence rate over the average.\n",
      "Evidence: Charter schools refers to `Charter School (Y/N)` = 1 in the table frpm; Excellence rate = NumGE1500 / NumTstTakr\n"
     ]
    }
   ],
   "source": [
    "# Define test cases for BIRD dataset\n",
    "bird_test_cases = [\n",
    "    # Test case 1: Excellence rate calculation (basic join with aggregation)\n",
    "    {\n",
    "        \"db_id\": \"california_schools\",\n",
    "        \"query\": \"List school names of charter schools with an SAT excellence rate over the average.\",\n",
    "        \"evidence\": \"Charter schools refers to `Charter School (Y/N)` = 1 in the table frpm; Excellence rate = NumGE1500 / NumTstTakr\"\n",
    "    },\n",
    "    \n",
    "    # Test case 2: Multi-table query with numeric conditions (multiple joins)\n",
    "    {\n",
    "        \"db_id\": \"game_injury\",\n",
    "        \"query\": \"Show the names of players who have been injured for more than 3 matches in the 2010 season.\",\n",
    "        \"evidence\": \"Season info is in the game table with year 2010; injury severity is measured by the number of matches a player misses.\"\n",
    "    },\n",
    "    \n",
    "    # Test case 3: Complex aggregation with grouping\n",
    "    {\n",
    "        \"db_id\": \"formula_1\",\n",
    "        \"query\": \"What is the name of the driver who has won the most races in rainy conditions?\",\n",
    "        \"evidence\": \"Weather conditions are recorded in the races table; winner information is in the results table.\"\n",
    "    },\n",
    "    \n",
    "    # Test case 4: Temporal query with date handling\n",
    "    {\n",
    "        \"db_id\": \"loan_data\",\n",
    "        \"query\": \"Find the customer with the highest total payment amount for loans taken in the first quarter of 2011.\",\n",
    "        \"evidence\": \"First quarter means January to March (months 1-3); loan dates are stored in ISO format (YYYY-MM-DD).\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Select the test case to run (0-3)\n",
    "test_idx = 0\n",
    "current_test = bird_test_cases[test_idx]\n",
    "\n",
    "print(f\"Selected test case {test_idx + 1}:\")\n",
    "print(f\"Database: {current_test['db_id']}\")\n",
    "print(f\"Query: {current_test['query']}\")\n",
    "print(f\"Evidence: {current_test['evidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a294b9bc-1135-4088-bd4b-7c85c1405521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEXT-TO-SQL PIPELINE EXECUTION\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "STEP 1: SCHEMA SELECTION\n",
      "==================================================\n",
      "[Step 1] Starting schema selection for database 'california_schools'\n",
      "[Step 1] Query: List school names of charter schools with an SAT excellence rate over the average.\n",
      "[Step 1] Evidence: Charter schools refers to `Charter School (Y/N)` = 1 in the table frpm; Excellence rate = NumGE1500 / NumTstTakr\n",
      "[Step 1] Requesting schema selection (timeout: 120s)\n",
      "[Tool] Loading schema for database: california_schools\n",
      "[Step 1] Schema selected successfully (took 1.5s)\n",
      "\n",
      "Schema selection result summary:\n",
      "----------------------------------------\n",
      "✓ Schema information successfully extracted\n",
      "\n",
      "Preview:\n",
      "{\"db_id\": \"california_schools\", \"table_count\": 3, \"total_column_count\": 89, \"avg_column_count\": 29, \"is_complex_schema\": true, \"full_schema_str\": \"<database_schema>\\n  <table name=\\\"frpm\\\">\\n    <colu...\n",
      "\n",
      "==================================================\n",
      "STEP 2: SQL GENERATION\n",
      "==================================================\n",
      "\n",
      "[Step 2] Starting SQL generation\n",
      "[Step 2] Query: List school names of charter schools with an SAT excellence rate over the average.\n",
      "[Step 2] Warning: Could not extract schema from selector output\n",
      "[Step 2] Requesting SQL generation (timeout: 120s)\n",
      "[Step 2] SQL generation completed (took 5.9s)\n",
      "[Step 2] SQL generated: SELECT T3.`County Name`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "JOIN schools...\n",
      "\n",
      "SQL generation result summary:\n",
      "----------------------------------------\n",
      "✓ SQL query generated (242 chars)\n",
      "\n",
      "SQL Query:\n",
      "SELECT T3.`County Name`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "JOIN schools AS T3 ON T1.`CDSCode` = T3.`CDSCode`\n",
      "WHERE T1.`Charter School (Y/N)` = 1\n",
      "GROUP BY T3.`County Name`\n",
      "ORDER BY AVG(T2.`AvgScrMath`) DESC\n",
      "LIMIT 1\n",
      "\n",
      "==================================================\n",
      "STEP 3: SQL REFINEMENT\n",
      "==================================================\n",
      "\n",
      "[Step 3] Starting SQL refinement\n",
      "[Step 3] Initial SQL: SELECT T3.`County Name`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "JOIN schools...\n",
      "[Step 3] Starting refinement attempt 1/3\n",
      "[Tool] Executing SQL on database california_schools: SELECT T3.`Name`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "JOIN schools AS T3 ...\n",
      "[SQLExecutor] Connecting to database: ../data/bird/dev_databases/california_schools/california_schools.sqlite\n",
      "[Step 3] Refinement attempt 1, status: \n",
      "[Step 3] New SQL detected: SELECT T3.`Name`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "JOIN schools AS T3 ...\n",
      "[Step 3] Starting refinement attempt 2/3\n",
      "[Tool] Executing SQL on database california_schools: PRAGMA table_info(schools);...\n",
      "[SQLExecutor] Connecting to database: ../data/bird/dev_databases/california_schools/california_schools.sqlite\n",
      "[Tool] Executing SQL on database california_schools: PRAGMA table_info(frpm);...\n",
      "[SQLExecutor] Connecting to database: ../data/bird/dev_databases/california_schools/california_schools.sqlite\n",
      "[Tool] Executing SQL on database california_schools: PRAGMA table_info(satscores);...\n",
      "[SQLExecutor] Connecting to database: ../data/bird/dev_databases/california_schools/california_schools.sqlite\n",
      "Error parsing JSON: Extra data: line 2 column 1 (char 518)\n",
      "[Step 3] Refinement attempt 2, status: \n",
      "Error parsing JSON: Extra data: line 2 column 1 (char 518)\n",
      "[Step 3] No SQL found in refinement output, attempt 2\n",
      "[Step 3] Starting refinement attempt 3/3\n",
      "[Tool] Executing SQL on database california_schools: SELECT T2.`sname`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "WHERE T1.`Charter ...\n",
      "[SQLExecutor] Connecting to database: ../data/bird/dev_databases/california_schools/california_schools.sqlite\n",
      "[Step 3] Refinement attempt 3, status: \n",
      "[Step 3] New SQL detected: SELECT T2.`sname`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "WHERE T1.`Charter ...\n",
      "[Step 3] Max refinements (3) reached\n",
      "[Step 3] Refinement complete - Final status: UNKNOWN\n",
      "[Step 3] Final SQL: SELECT T2.`sname`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "WHERE T1.`Charter ...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT\n",
      "==================================================\n",
      "Database: california_schools\n",
      "Query: List school names of charter schools with an SAT excellence rate over the average.\n",
      "\n",
      "Execution Status: UNKNOWN\n",
      "\n",
      "Final SQL Query:\n",
      "SELECT T2.`sname`\n",
      "FROM frpm AS T1\n",
      "JOIN satscores AS T2 ON T1.`CDSCode` = T2.`cds`\n",
      "WHERE T1.`Charter School (Y/N)` = 1\n",
      "AND (T2.`NumGE1500` / T2.`NumTstTakr`) > (\n",
      "    SELECT AVG(T2_sub.`NumGE1500` / T2_sub.`NumTstTakr`)\n",
      "    FROM satscores AS T2_sub\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Execute each step individually with proper error handling and timeout management\n",
    "try:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT-TO-SQL PIPELINE EXECUTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Schema Selection\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 1: SCHEMA SELECTION\")\n",
    "    print(\"=\"*50)\n",
    "    task, selector_content = await select_schema(json.dumps(current_test))\n",
    "    \n",
    "    print(\"\\nSchema selection result summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    # Verify if we got schema information\n",
    "    if \"<database_schema>\" in selector_content or \"schema_str\" in selector_content:\n",
    "        print(\"✓ Schema information successfully extracted\")\n",
    "    else:\n",
    "        print(\"⚠ Schema information may be missing or malformed\")\n",
    "    # Print a preview of the content\n",
    "    print(\"\\nPreview:\")\n",
    "    preview = selector_content[:200] + \"...\" if len(selector_content) > 200 else selector_content\n",
    "    print(preview)\n",
    "\n",
    "    # Step 2: SQL Generation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 2: SQL GENERATION\")\n",
    "    print(\"=\"*50)\n",
    "    decomposer_content, sql = await generate_sql(selector_content, task)\n",
    "    \n",
    "    print(\"\\nSQL generation result summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    if sql:\n",
    "        print(f\"✓ SQL query generated ({len(sql)} chars)\")\n",
    "        print(\"\\nSQL Query:\")\n",
    "        print(sql[:300] + \"...\" if len(sql) > 300 else sql)\n",
    "    else:\n",
    "        print(\"⚠ No SQL query was generated\")\n",
    "        print(\"\\nDecomposer output preview:\")\n",
    "        print(decomposer_content[:200] + \"...\" if len(decomposer_content) > 200 else decomposer_content)\n",
    "\n",
    "    # Step 3: SQL Refinement\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 3: SQL REFINEMENT\")\n",
    "    print(\"=\"*50)\n",
    "    if not sql:\n",
    "        print(\"\\n⚠ Skipping refinement because no SQL was generated\")\n",
    "        result = {\n",
    "            \"db_id\": task.get('db_id', ''),\n",
    "            \"query\": task.get('query', ''),\n",
    "            \"final_output\": decomposer_content,\n",
    "            \"error\": \"No SQL generated\",\n",
    "            \"status\": \"ERROR_NO_SQL\"\n",
    "        }\n",
    "    else:\n",
    "        result = await refine_sql(decomposer_content, sql, task)\n",
    "    \n",
    "    # Display final result\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Display database and query info\n",
    "    print(f\"Database: {result.get('db_id', '')}\")\n",
    "    print(f\"Query: {result.get('query', '')}\")\n",
    "    \n",
    "    # Try to parse and format the result\n",
    "    try:\n",
    "        final_output = parse_json(result.get(\"final_output\", \"{}\"))\n",
    "        \n",
    "        # Display status\n",
    "        status = final_output.get(\"status\") or result.get(\"status\", \"UNKNOWN\")\n",
    "        print(f\"\\nExecution Status: {status}\")\n",
    "        \n",
    "        # Display final SQL\n",
    "        final_sql = final_output.get(\"final_sql\") or result.get(\"final_sql\", \"\")\n",
    "        if final_sql:\n",
    "            print(f\"\\nFinal SQL Query:\")\n",
    "            print(final_sql)\n",
    "        else:\n",
    "            print(\"\\n⚠ No final SQL query available\")\n",
    "            \n",
    "        # Display execution result if available\n",
    "        if \"execution_result\" in final_output:\n",
    "            print(\"\\nExecution Result:\")\n",
    "            exec_result = final_output[\"execution_result\"]\n",
    "            if isinstance(exec_result, dict):\n",
    "                for key, value in exec_result.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(exec_result)\n",
    "                \n",
    "        # Display any errors\n",
    "        if \"error\" in final_output or \"error\" in result:\n",
    "            error = final_output.get(\"error\") or result.get(\"error\", \"\")\n",
    "            print(f\"\\nError: {error}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError parsing final result: {str(e)}\")\n",
    "        if \"final_sql\" in result:\n",
    "            print(f\"\\nFinal SQL:\\n{result['final_sql']}\")\n",
    "        print(\"\\nRaw output preview:\")\n",
    "        raw_output = result.get(\"final_output\", \"\")\n",
    "        print(raw_output[:500] + \"...\" if len(raw_output) > 500 else raw_output)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nCRITICAL ERROR IN EXECUTION: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
